<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.6.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2020-11-01T15:42:35-05:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Data Science Portfolio</title><subtitle>Created by Anthony Lipphardt as a portfolio for data science projects, this site includes samples of work that may encompass data visualization, programming, and web development.</subtitle><author><name>Anthony Lipphardt</name></author><entry><title type="html">Clumsy Movie Nights with a Discord Bot</title><link href="http://localhost:4000/2020/10/17/Discord-Bot.html" rel="alternate" type="text/html" title="Clumsy Movie Nights with a Discord Bot" /><published>2020-10-17T00:00:00-04:00</published><updated>2020-10-17T00:00:00-04:00</updated><id>http://localhost:4000/2020/10/17/Discord-Bot</id><content type="html" xml:base="http://localhost:4000/2020/10/17/Discord-Bot.html">&lt;p&gt;&lt;a href=&quot;https://github.com/alipphardt/clumsy-movie-bot&quot;&gt;GitHub Repo&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;background&quot;&gt;Background&lt;/h2&gt;

&lt;p&gt;Saturday night movies via Discord have been a guilty pleasure among friends during quarantine, starting with well known cult films such as &lt;a href=&quot;https://www.imdb.com/title/tt0368226/&quot;&gt;The Room&lt;/a&gt; or &lt;a href=&quot;https://www.imdb.com/title/tt0092549/&quot;&gt;Miami Connection&lt;/a&gt; and delving more into the weird over time. As the selection of movies have broadened its come to be known as ‘Clumsy’ Movie Night, a term that was coined by my daughter.&lt;/p&gt;

&lt;p&gt;Nominations and voting take place on a dedicated Discord text channel, with movie titles submitted as messages and votes submitted through reactions or emojis. In the early days of clumsy movie night, there may have been up to 10 movies at at a time where votes were tallied in an Excel sheet before being populated into a wheel for random selection (&lt;a href=&quot;https://wheelofnames.com&quot;&gt;wheelofnames.com&lt;/a&gt;). Number of entries on the wheel for a given movie are equal to the number of votes received. The first movie to be selected three times is chosen as the movie to be shown that night.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/clumsy-movies-bot/samples.png&quot; alt=&quot;Sample votes in Discord channel&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/clumsy-movies-bot/wheelofnames.png&quot; alt=&quot;Wheel populated with sample movie titles&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Eventually as more members were added to the Discord channel nominations increased to up to 50 movie titles in a given week, which became unsustainable for manual tallies. Enter the Clumsy Movie Bot, whose likeness was pulled from the killer robot in the 1990 sci-fi thriller, &lt;a href=&quot;https://www.imdb.com/title/tt0099740&quot;&gt;Hardware&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/clumsy-movies-bot/hardware-robot.jpg&quot; alt=&quot;Robot from 1990 movie Hardware&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;implementation-with-python&quot;&gt;Implementation with Python&lt;/h2&gt;

&lt;p&gt;The bot was developed in Python and primarily uses two libraries for its commands, the &lt;strong&gt;discordpy&lt;/strong&gt; and &lt;strong&gt;imdbpy&lt;/strong&gt; libraries.&lt;/p&gt;

&lt;p&gt;The commands for the bot are broken into three main categories:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Voting commands facilitate the tallying of votes based on reactions/emojis, printing a list of movie titles to be copy/pasted into &lt;a href=&quot;https://wheelofnames.com&quot;&gt;wheelofnames.com&lt;/a&gt;, helper commands for adding winning movies to temporary or permanent lists, as well as the ability to create a list of rollover titles for the following week for movies not in the winners list.&lt;/li&gt;
  &lt;li&gt;IMDB commands integrate with IMDB.com to run searches for specified titles against the IMDB database. Results returned may be used to add movies into a permanent list of winners, to select movies from the Top 1000 b-movies at random, or to generate trivia for the current winning movie.&lt;/li&gt;
  &lt;li&gt;Utility commands are used strictly in testing/development. This includes generating sample movie titles with votes, the ability to purge messages from the testing channel, and a user command to force the bot to logout.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;For the purposes of documentation, the python scripts are managed in a Jupyter notebook and run from a local laptop, with future plans to run from a Raspberry Pi in order to keep the bot available around the clock.&lt;/p&gt;

&lt;p&gt;All commands are implemented through the use of python decorator functions to extend the command function in the Commands class of the &lt;strong&gt;imdbpy&lt;/strong&gt; library, with a custom check implemented to ensure that commands are only accepted from specific channels in the testing or production Discord servers.&lt;/p&gt;

&lt;h2 id=&quot;sample-commands&quot;&gt;Sample Commands&lt;/h2&gt;

&lt;p&gt;The following are sample outputs from several of the basic commands.&lt;/p&gt;

&lt;p&gt;The &lt;strong&gt;tally&lt;/strong&gt; command tallys all votes that occurred since the previous Saturday at 10 PM, creates and saves a Matplotlib bar chart of the votes, and then submits the message to the current channel attaching the plot as an embedded image.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/clumsy-movies-bot/tally.png&quot; alt=&quot;Tally of sample votes&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The &lt;strong&gt;.wheel&lt;/strong&gt; command takes the list of vote tallies and prints the titles multiple times according to the number of votes received. This provides a list that is easy for the user to copy and paste into the wheel before spinning.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/clumsy-movies-bot/wheel.png&quot; alt=&quot;List of movie titles proportional to number of votes&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The &lt;strong&gt;.winners&lt;/strong&gt; command prints a list of all movies previously selected on Clumsy Movie Night. The titles have associated numbers which may be used with the IMDB trivia command.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/clumsy-movies-bot/winners.png&quot; alt=&quot;List of prior winners&quot; /&gt;&lt;/p&gt;

&lt;p&gt;By issuing the &lt;strong&gt;.trivia&lt;/strong&gt; command with the specified index, the bot will pull top 10 trivia for the winning movie from IMDB and print the results to the current channel.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/clumsy-movies-bot/trivia.png&quot; alt=&quot;Sample trivia for The Room&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Searches on IMDB may also be performed using the &lt;strong&gt;.imdb&lt;/strong&gt; command. Given that multiple movies may be returned with similar names, the default behavior is to return a list of numbered options.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/clumsy-movies-bot/imdb.png&quot; alt=&quot;IMDB query for the Matrix&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Running a followup command &lt;strong&gt;.imdb_summary&lt;/strong&gt; with the specified index will display an IMDB summary with the title, description, IMDB score, running time, and movie poster.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/clumsy-movies-bot/imdb_summary.png&quot; alt=&quot;IMDB summary for The Matrix&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Finally, a random movie by bot selection is also available to provide an additional level of chaos. By running the &lt;strong&gt;.random&lt;/strong&gt; command, a random movie is selected from the Top 1000 movies on IMDB with keyword b-movie.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/clumsy-movies-bot/random.png&quot; alt=&quot;Random movie from IMDB&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;creating-the-bot-account-on-discord&quot;&gt;Creating the bot account on Discord&lt;/h2&gt;

&lt;p&gt;In order to create a dedicated account for the bot, login to Discord via &lt;a href=&quot;https://discord.com/developers/applications&quot;&gt;https://discord.com/developers/applications&lt;/a&gt;. Under &lt;strong&gt;General Information&lt;/strong&gt;, create a new application specifying a name, description, and app icon for the application. Then go to the menu labelled &lt;strong&gt;Bot&lt;/strong&gt; and select to Add Bot. This will convert the application to an account that may connect to Discord similar to a regular user.&lt;/p&gt;

&lt;p&gt;In order to add the bot as an accepted user to a given channel, the server admin must accept an invite that may be sent in the form of an OAuth2 URL. This URL can be created by going to the &lt;strong&gt;OAuth2&lt;/strong&gt; menu, selecting ‘Bot’ from the list of Scopes and then assigning the following permissions:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/clumsy-movies-bot/scope.png&quot; alt=&quot;Screenshot of scope for bot&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/clumsy-movies-bot/permissions.png&quot; alt=&quot;Screenshot of permissions for bot&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Once all of the desired permissions have been set, the admin accesses the OAuth2 URL and accepts the invite, allowing the bot access with the specified permissions to the Discord channel.&lt;/p&gt;

&lt;p&gt;Running the python script or Jupyter notebook will then initialize the bot with the custom commands and connects it to Discord. From there the bot is ready to receive commands from any user in the channel.&lt;/p&gt;

&lt;h2 id=&quot;considerations-and-next-steps&quot;&gt;Considerations and Next Steps&lt;/h2&gt;

&lt;p&gt;The downside to the current setup is that scripts running from a local laptop may be interrupted when the machine is shutdown, disconnecting the bot from the server. Therefore, next steps are to run the bot from a local server that retains a persistent connection to the Discord server. This can be setup through a Raspberry Pi. Additional options are third party services such as Heroku apps that run your application remotely. This will require some modifications as the current script is set up to pull access tokens, channel IDs, and so forth from environmental variables on the local computer.&lt;/p&gt;

&lt;p&gt;Another area of focus is some exploratory analyses of movies selected using IMDB database information. Findings may be used to inform the development of an improved recommender system that will suggest movies similar to titles chosen in the past, rather than simply selecting randomly from a top 1000 list.&lt;/p&gt;

&lt;p&gt;For example, a quick analyses of keywords for all prior winners shows that the majority of movies have keywords associated with cult-film, psychotronic-film, or surrealism. Using top keywords as selected features might provide a means to generate a training set for supervised machine learning, allowing the bot to select new movies that meet a combination of keyword criteria. This could also be coupled with numeric scores based on how much the audience enjoyed each given movie, which could be implemented with some custom scoring commands.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/clumsy-movies-bot/imdb-keywords-word-cloud.png&quot; alt=&quot;Word cloud of top keywords for winning movies&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The imdbpy package also has utility functions to download the latest version of the IMDB database for local SQL queries. This could be used to develop more complex queries than what is capable from the imdbpy API. For example, select all movies where the lead actor served as writer and director (e.g. Tommy Wiseau in The Room).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/clumsy-movies-bot/the-room.gif&quot; alt=&quot;Tommy Wiseau&quot; /&gt;&lt;/p&gt;</content><author><name>Anthony Lipphardt</name></author><summary type="html">GitHub Repo</summary></entry><entry><title type="html">Automated Identification and Coding of Drug Overdose Records</title><link href="http://localhost:4000/2019/04/23/drug-overdose-records.html" rel="alternate" type="text/html" title=" Automated Identification and Coding of Drug Overdose Records" /><published>2019-04-23T00:00:00-04:00</published><updated>2019-04-23T00:00:00-04:00</updated><id>http://localhost:4000/2019/04/23/drug-overdose-records</id><content type="html" xml:base="http://localhost:4000/2019/04/23/drug-overdose-records.html">&lt;h2 id=&quot;background&quot;&gt;Background&lt;/h2&gt;

&lt;p&gt;The National Center for Health Statistics (NCHS), part of the Centers for Disease Control and Prevention (CDC), is a principal agency of the Federal Statistical System which provides statistical information to guide actions and policies to improve the health of the American people. NCHS oversees a number of surveys and data collection systems, including the National Vital Statistics System (NVSS). The NVSS works in partnership with state and local governments to collect, analyze, and disseminate data on vital statistics, including birth and death events. This data is used to monitor trends of public health importance such as the ongoing opioid crisis.&lt;/p&gt;

&lt;p&gt;Mortality data from the NVSS is currently coded using the International Classification of Diseases, Tenth Revision (ICD-10), which is used to identify underlying and contributory causes of death. Coding of records for drug involved deaths in particular is a complex process and faces unique challenges, including:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Manual coding:&lt;/strong&gt; NCHS uses automated programs to assign ICD-10 codes for as many as twenty multiple causes of death and one underlying cause of death based on a complex set of rules. In general, the program rejects about one-fifth of death records which must be reviewed and coded by trained nosologists. However, for the estimated 2.6 percent of deaths with an underlying cause of drug overdose, about two-thirds of records are rejected and require manual coding (Trinidad, Warner, Bastian, Minino &amp;amp; Hedegaard, 2016).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Incomplete information:&lt;/strong&gt; Approximately 20 percent of drug overdose records do not include specific drug mentions (Rudd, Seth, David, &amp;amp; Scholl, 2016). This makes it difficult to categorize records as ICD-10 codes are based on specific drugs or broad drug classes.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Limited codes for specific drugs:&lt;/strong&gt; ICD-10 is limited in capturing drug-involved mortality as it contains few codes pertaining to specific drugs (e.g. heroin, methadone, cocaine) and these codes are only applied in specific circumstances. Most drugs are coded using broad categories or drug classes, making it difficult to monitor trends in specific drugs that are not already uniquely classified by ICD-10.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The current state of the organization allows for release of final mortality data once it has been cleaned and coded, approximately 12-months after data collection. Timely release of mortality data for drug-involved deaths is of critical importance, with the opioid crisis currently recognized as a national public health emergency (The White House, 2018). The CDC (2019) reported that in 2017, the final number of overdose deaths involving opioids was 6 times higher than in 1999. More specifically, the age-adjusted death rate due to poisoning with synthetic opioids (e.g., fentanyl) has increased by a factor of 30. This trend illustrates the need to better understand the specific drugs involved within this broad category.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/capstone/drug-class-trends.png&quot; alt=&quot;drug trends by drug class&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;prior-work&quot;&gt;Prior Work&lt;/h2&gt;

&lt;p&gt;Past work by Trinidad, Warner, Bastian, Minino and Hedegaard (2016) have focused on developing programs to identify records containing drug mentions with involvement (DMI) and to extract specific drug mentions from literal text fields of the U.S. Standard Certificate of Death. These fields include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The chain of events leading to death (from Part I)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Other significant conditions that contributed to cause of death (from Part II)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;How the injury occurred (in the case of deaths due to injuries from Box 43)&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Programs developed rely on exact pattern matching and significant collaboration between NCHS and the Food and Drug Administration (FDA) to develop search terms for identified drug mentions, descriptors (e.g., illicit, prescription/RX), and contextual phrases (e.g., ingested, history/HX of abuse) (Trinidad et. al, 2016). Analysis of literal text in this manner is beneficial as it provides an opportunity for an enhanced understanding of the national picture of drug involvement in deaths in the United States. However, the time involved to develop and maintain keyword lists and programs necessary for exact pattern matching is problematic.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/capstone/literal-text-example.png&quot; alt=&quot;literal text example&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/2018/06/11/DMI-Deaths.html&quot;&gt;Previous work&lt;/a&gt; as part of Loyola’s graduate course in Machine Learning explored the use of literal text to identify DMI records with high precision and specificity. Within the context of the project, a record was determined to include a DMI if the underlying cause or any of the twenty available multiple causes were assigned a code pertaining to drug overdose (i.e. X40-44, X60-64, X85, Y10-Y14). Results from the program detailed by Trinidad et al. (2016) identified DMI records with a positive predictive rate or precision of 95.8 percent. Using this as a baseline, a program was created in Python using basic natural language processing (NLP) techniques to convert a corpus of literal text records into a sparse document term matrix to train a classifier to identify DMI records. The program  exceeded the baseline with a precision and specificity for DMI records of 98.6 percent. It should be noted that the results in the program were achieved using publicly available data from the Washington State Department of Health and may not completely reflect the quality of text data from all states and territories. This project will develop a similar model using data for the United States and territories from 2005-2017 available to NCHS.&lt;/p&gt;

&lt;h2 id=&quot;research-questions&quot;&gt;Research Questions&lt;/h2&gt;
&lt;p&gt;The focus of this capstone project is centered on three main questions given the problem as defined:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Can we use machine learning to accurately identify records with an underlying cause of drug overdose?&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Can we extend those methods to assign specific ICD-10 codes?&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Are there better methods for extracting drug mentions, including novel drugs not yet added to expert lists?&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;data-sources&quot;&gt;Data Sources&lt;/h2&gt;
&lt;p&gt;The project uses two primary data sources from the NCHS’ National Vital Statistics System. Historic mortality data from 2005-2017 and literal text data files.&lt;/p&gt;

&lt;p&gt;The mortality data file includes records of death from all 50 states and territories through 2017. Information in the mortality data file includes general demographics for the decedent, data year, and a variety of codes pertaining to a single underlying cause of death and up to 20 multiple causes of death. Detailed documentation on the public use data files are provided by NCHS (2018). Public use files are provided by year and fields may be added or removed over time.  For the purpose of this project, the specific fields requested were ICD-10 codes pertaining to underlying and multiple causes of death and the manner of death field which is related to the intent of death. All other fields were dropped from the provided data files.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;ICD-10 Codes
    &lt;ul&gt;
      &lt;li&gt;Multiple causes of death&lt;/li&gt;
      &lt;li&gt;Underlying cause of death&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Manner of death
    &lt;ul&gt;
      &lt;li&gt;Accidental&lt;/li&gt;
      &lt;li&gt;Intentional&lt;/li&gt;
      &lt;li&gt;Homicide&lt;/li&gt;
      &lt;li&gt;Undetermined&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/images/capstone/number-of-records.png&quot; alt=&quot;number of records in mortality file&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The literal text files include text as entered by a physician, medical examiner, or coroner on the U.S. Standard Certificate of Death. Variables from the literal text file include the chain of events leading to death (from Part I, Lines A through D), time intervals for each event in Part I, other significant conditions that contributed to cause of death (from Part II), how the injury occurred (in the case of deaths due to injuries from Box 43), and place of injury. For the purpose of this project, only fields containing literal text for Part I, II, and Box 43 were requested. All other fields from the literal text files were dropped.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/capstone/cause-of-death.png&quot; alt=&quot;cause of death section of the u.s. standard death certificate&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/capstone/literal-text-dataset.png&quot; alt=&quot;literal-text-dataset&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;a-quick-primer-on-icd-10-codes&quot;&gt;A Quick Primer on ICD-10 Codes&lt;/h2&gt;

&lt;p&gt;Multiple cause codes within the mortality file may be used to indicate cause of death as well as specific substances or broad drug classes contributing to the cause(s) of death.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;ICD-10 codes for drug overdose as underlying cause of death are characterized by intent and mechanism:
    &lt;ul&gt;
      &lt;li&gt;X40-X44&lt;/li&gt;
      &lt;li&gt;X60-X64&lt;/li&gt;
      &lt;li&gt;X85&lt;/li&gt;
      &lt;li&gt;Y10-Y14&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Additional T, F, or R codes may indicate specific substances or broad drug classes
    &lt;ul&gt;
      &lt;li&gt;T40.1 - Heroin&lt;/li&gt;
      &lt;li&gt;T40.4 - Synthetic Opioids&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Running some cross tabulations for records by ICD code and variables such as intent and mechanism, it is observed that there are significant class imbalances which must be accounted for when developing predictive models.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/capstone/icd-by-intent.png&quot; alt=&quot;icd-by-intent&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;primary-objective-identify-and-code-drug-overdose-records&quot;&gt;Primary Objective: Identify and Code Drug Overdose Records&lt;/h2&gt;

&lt;p&gt;Given the heavy class imbalance within the datasets, the metrics of interest for model comparison and selection will be precision and recall. Precision determines the proportion of records assigned to a given class that were correct. minimizing false positives. Recall determines the proportion of records belonging to a class that were correct, minimizing false negatives.&lt;/p&gt;

&lt;p&gt;Models with the best performance in each classifier were combined together to identify records with a status of drug overdose and based on the predicted intent and mechanism categories, assign an ICD-10 code from the subset X40-44, X60-64, X85, and Y10-Y14. The mapping of intent and mechanism categories to individual ICD-10 codes are provided within a lookup table for the final assignment of ICD code.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/capstone/classifiers.png&quot; alt=&quot;organization of classifiers in project&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;data-preprocessing&quot;&gt;Data Preprocessing&lt;/h3&gt;

&lt;p&gt;The following pre-processing steps were applied to mortality and literal text data files:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Add flag variables for drug overdose status, intent, mechanism&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Remove rows with missing or uninformative literal text&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Standardize text - All uppercase, stripped punctuation and numbers&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Concatenate text fields and convert to document term matrix (i.e. bag of words)&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/images/capstone/bag-of-words.png&quot; alt=&quot;bag of words transformation&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;sampling&quot;&gt;Sampling&lt;/h3&gt;
&lt;p&gt;The main limitation of the mortality file is the significant class imbalance between overdose (~500,000 records) and non-overdose (32 million records). To account for this imbalance, a stratified sample of 500,000 overdose and non-overdose records was taken, using simple random sampling without replacement. The records were then divided evenly into training and test sets. Due to class imbalance among specific ICD codes, under-represented classes were oversampled.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/capstone/train-test-sets.png&quot; alt=&quot;train and test sets in SAS&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;experimental-design&quot;&gt;Experimental Design&lt;/h3&gt;
&lt;p&gt;The Scikit-Learn package of Python was used to develop a machine learning pipeline for this project. The pipeline can be broken into three main categories:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/capstone/pipeline.png&quot; alt=&quot;Sci-kit learn pipeline&quot; /&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Vectorizer&lt;/strong&gt; - This initial step transforms the pre-processed data into a term-document matrix using a chosen vectorizer. Values could take on binary counts to indicate the presence or absence of a word in a given record, TF-IDF to indicate the relative importance of a word in a record relative to the entire corpus/dataset, or specialized hashing vectorizers which run the terms through a hashing function and indicate presence or absence of a hashed word. The hashing vectorizer removes the need for stored vocabularies saving storage, but can be problematic as multiple words resolve to the same value when passed through the hashing function. This also hinders downstream interpretability of the models. For example, examining coefficients of an SVM model may indicate which terms from the source vocabulary have the most predictive power for overdose or non-overdose status, shown in the below figure.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Classifier&lt;/strong&gt; - Methods such as Naive Bayes (NB) and Support Vector Machine (SVM) have been found to be successful for classifying records containing drug mentions with involvement (Lipphardt, 2018). Using the appropriate features, these linear classifiers have often been shown to perform on par with state-of-the-art algorithms in text classification (Wang &amp;amp; Manning, 2012). These are the primary algorithms that were considered within the scope of this project. Results from Wang and Manning (2012) also show that in general Multinomial Naive Bayes outperforms Multivariate Bernoulli Naive Bayes and that binarized values are better than standard counts (pg. 90). Based on these findings the Multinomial Naive Bayes with binarized values are chosen as the preferred model for Naive Bayes classifiers. SVM models are implemented with a linear kernel based on the fact that text classification problems are often found to be linearly separable given the high dimensionality of the feature space.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Result&lt;/strong&gt; - To determine optimal hyperparameters for machine learning models, a grid search in Scikit-Learn was used to test combinations of parameters, measuring performance using three fold cross validation. Grid search was configured to select optimal parameters based off a precision macro-average, which computes the unweighted mean precision among all class labels. Once optimal hyperparameters are chosen, model predictions are obtained on the test set to determine final precision scores.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;/images/capstone/svm-coefficients.png&quot; alt=&quot;SVM coefficients&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;overdose-status&quot;&gt;Overdose Status&lt;/h3&gt;
&lt;p&gt;The performance of the model developed in this capstone project is compared to the baseline detailed in work by Trinidad, Warner, Bastian, Minino &amp;amp; Hedegaard (2016). Compared to the baseline model, the capstone models based on machine learning SVM classifiers outperform with a precision score of 99.7 percent.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Baseline Model
    &lt;ul&gt;
      &lt;li&gt;Exact Term Matching&lt;/li&gt;
      &lt;li&gt;2013 data year&lt;/li&gt;
      &lt;li&gt;2,000 records&lt;/li&gt;
      &lt;li&gt;Precision: 95.8%&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Capstone Model
    &lt;ul&gt;
      &lt;li&gt;SVM Classifier&lt;/li&gt;
      &lt;li&gt;2005-2017 data years&lt;/li&gt;
      &lt;li&gt;499,187 records&lt;/li&gt;
      &lt;li&gt;Precision: 99.7%&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/images/capstone/confusion-matrix-od-status.png&quot; alt=&quot;confusion matrix for overdose status prediction&quot; /&gt;&lt;/p&gt;

&lt;p&gt;These results support a potential use case for developing near real-time provisional counts of drug overdose deaths, not taking into account individual ICD-10 codes or drug clases. It should be noted that this model does not consider or determine multiple cause codes, which are an important component of the final mortality file. However, given that more than two thirds of drug overdose records must be coded manually, this method would provide a rapid way of flagging records for manual review.&lt;/p&gt;

&lt;p&gt;Organizations working primarily in SAS may conder that Visual Data Mining and Machine Learning product within the SAS Viya platform to develop these machine learning pipelines. Supported CAS libraries or APIs allow integration of Python and R models into the data pipeline.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/capstone/sas-vdmml.png&quot; alt=&quot;SAS VDMML pipeline&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;individual-icd-10-codes&quot;&gt;Individual ICD-10 Codes&lt;/h3&gt;
&lt;p&gt;Using the best models or heuristics for overdose status, intent, and mechanism, ICD-10 codes were obtained using the sample of 499,187 records split evenly between overdose and non-overdose deaths.&lt;/p&gt;

&lt;p&gt;Classification on individual ICD-10 codes resulted in a precision weighted macro-average of 0.9627, with precision scores for individual codes ranging from 0.2613 to 0.9586. Recall weighted macro-average was 0.9590, with recall scores for individual classes ranging from 0.81 to 0.9926.&lt;/p&gt;

&lt;p&gt;From these results it can be observed that the model exhibits good sensitivity and typically predicts class labels accurately for more than 95 percent of the records it is passed. However, given the heavy class imbalance, precision scores for ICD-10 codes with little support suffer when the model assigns false positives.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Weighted macro avg.
    &lt;ul&gt;
      &lt;li&gt;Precision: 96.2%&lt;/li&gt;
      &lt;li&gt;Recall: 95.9%&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Unweighted macro avg.
    &lt;ul&gt;
      &lt;li&gt;Precision: 74.0%&lt;/li&gt;
      &lt;li&gt;Recall: 92.4%&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/images/capstone/icd-code.png&quot; alt=&quot;ICD&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Future work for predictive models may focus on improvements towards mechanism classifier performance, which exhibits low support for select ICD-10 codesw. Improvements might be gained through additional pre-processing - reducing the number of terms considered - and modern techniques such as deep neural networks (e.g. RNN, LSTM) which take advantage of context provided by word order.&lt;/p&gt;

&lt;p&gt;For the purposes of this capstone project, techniques or toolsets were constrained by the amount of training data and availablity of processing power or toolsets on NCHS’ consolidated statistical platform. As additional processing power and toolsets become on the platform, deep learning models may be incorporated.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/capstone/icd-by-mechanism.png&quot; alt=&quot;ICD by mechanism&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;secondary-objective-extract-known-and-novel-drug-mentions&quot;&gt;Secondary Objective: Extract Known and Novel Drug Mentions&lt;/h2&gt;

&lt;p&gt;As discussed, prior work by Trinidad et al. (2016) has focused on the use of SAS programs to identify drug mentions with involvement (DMI) within literal text fields via exact matches and regular expressions.&lt;/p&gt;

&lt;p&gt;Trinidad et al. (2016) note:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;The importance of creating comprehensive lists to be used by the DMI programs cannot be overstated. The DMI programs are a series of steps that identify drug mentions, descriptors of those drug mentions, and other contextual information. Each step of the processing of literal text requires lists: search terms, descriptors, joining phrases, or contextual phrases. Incomplete lists used by the DMI programs may result in failure in any of the processing steps, which would result in a failure to associate drug mentions with the appropriate contextual information.&lt;/em&gt; (p. 13)&lt;/p&gt;

&lt;p&gt;Although the programs are designed to obtain a high level of agreement with ICD-10 codes assigned through manual review, the process of developing precompiled lists is extremely time intensive and may miss drugs rarely involved in drug overdose deaths as well as uncommon abbreviations or misspellings. Future work is focused on identifying not only known drug mentions, but emerging trends in drug mortality such as the introduction of new fentanyl analogs. In order to improve the quality of reporting for drug overdose deaths, it is desirable to develop a model that can automatically extract both known and novel drug mentions.&lt;/p&gt;

&lt;p&gt;One method considered was to expand the lists maintained for the current NCHS DMI programs using more comprehensive vocabularies such as the Unified Medical Language System (UMLS), which provides access to a metathesaurus containing terms across multiple standards such as ICD-10, MeSH, RxNorm, and SNOMED. This method would potentially miss acronyms, abbreviations, and misspellings. Research from Tolentino et al. has focused on incorporating numerous algorithms to identify and correct these issues in a pre-processing step, but result in low sensitivity and precision (2007). The system also has the potential to miss newly emerging drugs or slang terms.&lt;/p&gt;

&lt;p&gt;An alternative method using unsupervised machine learning comes from research by Simpson, Adams, Brugman, and Conners (2018), which explored the use of word embeddings to detect novel and emerging drug mentions in social media. The original paper explores the use of the word2vec algorithm to identify novel drug mentions for the target substance ‘marijuana’ in social media by querying the model for top terms according to cosine similarity. Candidate terms were then manually reviewed to verify novel mentions. A semi-automated version of this approach may be used to identify known drug mentions by calculating cosine similarity between each word in a record and one or more selected target words, flagging those words which exceed an optimal threshold value. The fastText algorithm introduced by Bojanowski, Grave, Joulin, and Mikolov (2016) extends word2vec by considering not only individual words, but character n-grams within word boundaries. Word vectors for complete words are constructed by summing the character ngram word vectors together, allowing vectors to be generated for out-of-vocabulary words. This algorithm was also tested within the context of this project to identify potential misspellings or novel drug mentions.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/capstone/tensorflow-vectors.png&quot; alt=&quot;Word Vectors in Tensorflow&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;summary-of-word2vec&quot;&gt;Summary of Word2Vec&lt;/h3&gt;

&lt;p&gt;A complete review of the word2vec and fastText algorithms are outside the scope of this writeup. For detailed information, see original articles by &lt;a href=&quot;https://arxiv.org/pdf/1301.3781.pdf&quot;&gt;Mikolov et al (2013)&lt;/a&gt; and &lt;a href=&quot;https://arxiv.org/pdf/1607.04606.pdf&quot;&gt;Bojanowski et al (2017)&lt;/a&gt;. A brief overview of word2vec is provided below.&lt;/p&gt;

&lt;p&gt;word2vec comes in two flavors, using either a continuous bag of words (CBOW) or a skip-gram approach. The CBOW approach is used within this capstone project.&lt;/p&gt;

&lt;p&gt;CBOW trains a neural network model by passing as input a one hot vector for a word that occurs in a given context, passes it through a hidden layer, and attempts to predict the probability of all other words within the vocabulary occurring in the same context/window using a softmax classifier. The network is optimized to minimize the loss in predicting probabilities for context words in the softmax layer. Once this loss is minimized, values are obtained from the hidden layer, also known as the embedding layer, to construct an embedding matrix of word vectors. The resulting word vectors can be used to compare semantic similarity using various distance measures. Cosine similarity is used within this project to compare word similarity.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/capstone/word2vec-example.png&quot; alt=&quot;Word2Vec example&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;data-preprocessing-1&quot;&gt;Data Preprocessing&lt;/h3&gt;
&lt;p&gt;Provisioned data files for 2005 through 2017 were used to build the corpus for word embedding models, using a stratified sample of 500,000 overdose and non-overdose records each. Only literal text fields pertaining to Part I, II, and Box 43 of the standard certificate of death were retained. These fields were then unpivoted and stacked into a single column of lines of text. All lines of text were converted to uppercase.&lt;/p&gt;

&lt;p&gt;Using the NLTK Python package, each line was tokenized splitting on non-word characters to convert the corpus to a python list of tokenized sentences (i.e., a list of token lists). The Phrases model of the gensim package was then built using this list to detect common bigrams (i.e., adjacent word pairs) within the corpus. Hyperparameters for the minimum frequency of bigrams and the threshold for forming bigram phrases were initially kept at their defaults of 5 and 10, respectively. Once the phrases model was built, the list of tokenized sentences was passed through the phrases model to replace common pairs of tokens with detected phrases. Performance of models with and without bigrams were evaluated.&lt;/p&gt;

&lt;p&gt;To prevent unimportant numeric values from being included in the word embedding model, regular expressions were used to strip years prior to phrase detection and any remaining numbers following replacement of phrases. This allowed detected phrases such as ‘U_47700’ (a synthetic opioid) and ‘TYPE_2’ (relating to TYPE 2 DIABETES) to be included within the model.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/capstone/sample-literal-text.png&quot; alt=&quot;sample literal text&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/capstone/word-tokens.png&quot; alt=&quot;word tokens&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;task-1-measure-agreement-with-historical-records&quot;&gt;Task 1: Measure Agreement with Historical Records&lt;/h3&gt;
&lt;p&gt;The first task analyzes NCHS mortality and literal text records for data year 2017 using optimal word vectors to identify target substances based on cosine similarity. Flagged records only consider mentions of the drugs themselves and do not take into consideration contextual information such as degree of certainty (e.g. ‘PROBABLE’, ‘UNKNOWN’) or temporal relationships (e.g. ‘HISTORY’, ‘PAST’, ‘RECENT’, ‘REMOTE’).&lt;/p&gt;

&lt;p&gt;Detailed results of the analysis using word embedding models are provided in the table below and compared with the results of Trinidad et. al (2016) as the baseline.&lt;/p&gt;

&lt;p&gt;The first metric calculates precision, which finds the proportion of records with flagged mentions for the target substance that actually had an applicable ICD-10 code. Given that contextual information was ignored in this approach, performance was remarkably similar to the results by Trinidad et. al, resulting in precision scores between 91.5 and 98.0 percent.&lt;/p&gt;

&lt;p&gt;The second metric calculates sensitivity or recall, which finds the proportion of records assigned a code for the target substance which were identified. The approach using word embedding models performed on par with the DMI programs, resulting in sensitivity between 98.6 and 99.6 percent.&lt;/p&gt;

&lt;p&gt;Given the performance of this approach, it is possible that word embedding models may be designed as a reliable alternative method to identifying known drug mentions, despite the lack of contextual information required to rule out drug mentions without involvement.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/capstone/target-substances.png&quot; alt=&quot;target substances&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;task-2-search-for-novel-drug-terms&quot;&gt;Task 2: Search for Novel Drug Terms&lt;/h3&gt;

&lt;p&gt;The second task analyzes public records from Washington state for data year 2016 by comparing cosine similarity of each term or phrase to a list of common metabolites, precursors, and analogs used as search terms for identifying fentanyl.&lt;/p&gt;

&lt;p&gt;A listing of opioid and non-opioid terms were flagged using the word embedding models. Raw counts of the drug mentions were used to measure the rarity of a term within the dataset. Approximately 78 terms were identified in the dataset based on cosine similarity. 24 of these terms were opioid related and 54 terms were non-opioid terms. Drug terms comprised 72 of the 78 identified terms with non-drug terms including INTOXICATION, NEUROMYELITIS, PHLEGMASIA, NOR, SUNDAY, and VINYL.&lt;/p&gt;

&lt;p&gt;Top opioid terms based on counts include known drug mentions such as HEROIN, METHADONE, and FENTANYL. Drugs with smaller counts include infrequently cited metabolites and their typos (e.g. PARAFLOUROFENTANYL, FUNANYL) as well as lesser known illicit drugs (U_47700, U47700, 4_ANPP) not included on the expert list used by Trinidad et. al (2016). The addition of verified typos and fentanyl analogs resulted in a 27 percent increase in fentanyl related mentions.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/capstone/flagged-drug-mentions.png&quot; alt=&quot;flagged drug mentions&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Using this approach, a corpus of approximately 22,000 unique terms were narrowed down using automated tools to 78 terms requiring manual review and using raw counts provided a mechanism to quickly identify novel mentions. The limitations of this method are that word vectors are dependent on the size and quality of the training dataset and that false positives may be generated. Therefore, these techniques would be best served as a complementary method to existing search term list approaches to identify new candidate terms. This technique could easily be applied to the most recently available year of mortality and literal text data to identify terms similar to fentanyl.&lt;/p&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/capstone/summary.png&quot; alt=&quot;summary of results&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;p&gt;Aggarwal, C.C., &amp;amp; Zhai, C.X. (2012). A survey of text classification algorithms. In Mining text data (pp 163-222). Boston, MA: Springer.&lt;/p&gt;

&lt;p&gt;Bojanowski, P., Grave, E., Joulin, A., &amp;amp; Mikolov, T. (2016). Enriching word vectors with subword information. Retrieved from https://arxiv.org/pdf/1607.04606.pdf&lt;/p&gt;

&lt;p&gt;Cameron, H.M., &amp;amp; McGoogan, E. (1981). A prospective study of 1152 hospital autopsies: I. Inaccuracies in death certification. The journal of pathology, 133(4). doi: https://doi.org/10.1002/path.1711330402&lt;/p&gt;

&lt;p&gt;Centers for Disease Control and Prevention (CDC). (2018). Understanding the epidemic. Retrieved from: https://www.cdc.gov/drugoverdose/epidemic/index.html&lt;/p&gt;

&lt;p&gt;Hedegaard, H., Miniño, A.M., Warner, M. (2018). Drug overdose deaths in the United States, 1999–2017. NCHS Data Brief, 329. Hyattsville, MD: National Center for Health Statistics. Retrieved from 
https://www.cdc.gov/nchs/data/databriefs/db329-h.pdf&lt;/p&gt;

&lt;p&gt;Joulin, A., Grave, E., Bojanowski, P., &amp;amp; Mikolov, T. (2016). Bag of tricks for ef icient text classification. Retrieved 
from https://arxiv.org/pdf/1607.01759.pdf&lt;/p&gt;

&lt;p&gt;Lipphardt, A. (2018). Using cause-of-death literal text from the death certificate for classification. Retrieved from https://github.com/alipphardt/dmi-deaths-classification.&lt;/p&gt;

&lt;p&gt;Mikolov, T., Chen, K., Corrado, G., &amp;amp; Dean, J. (2013). Ef icient estimation of word representations in vector space. Retrieved from: https://arxiv.org/pdf/1301.3781.pdf&lt;/p&gt;

&lt;p&gt;National Center for Health Statistics (NCHS). (2018). Release of 2016 multiple cause-of-death data file. Available from: https://www.cdc.gov/nchs/data/dvs/Multiple_Cause_Record_Layout_2016.pdf&lt;/p&gt;

&lt;p&gt;Quoc, V.L., &amp;amp; Mikolov, T. (2014). Distributed representations of sentences and documents. Retrieved from: https://arxiv.org/pdf/1405.4053.pdf&lt;/p&gt;

&lt;p&gt;Simpson, S.S., Adams, N., Brugman, C.M., &amp;amp; Conners, T.J. (2018). Detecting novel and emerging drug terms using natural language processing: A social media corpus study. JMIR Public Health and Surveillance, 4(1).&lt;/p&gt;

&lt;p&gt;Spencer, M.R., Warner, M., Bastian, B.A., Trinidad, J.P., &amp;amp; Hedegaard, H. (2019) Drug overdose deaths involving fentanyl, 2011–2016. National vital statistics reports, 68(3). Retrieved from https://www.cdc.gov/nchs/data/nvsr/nvsr68/nvsr68_03-508.pdf&lt;/p&gt;

&lt;p&gt;Trinidad, J.P., Warner M., Bastian, B.A., Minino, A.M., &amp;amp; Hedegaard H. (2016). Using literal text from the death certificate to enhance mortality statistics: Characterizing drug involvement in deaths. National vital statistics reports, 65(9). Hyattsville, MD: National Center for Health Statistics.&lt;/p&gt;

&lt;p&gt;Tolentino, H.D., Matters, M. D., Walop, W., Law, B., Tong, W., Liu, F., Fontelo, P., Kohl, K., &amp;amp; Payne, D.C. (2007). A UMLS-based spell checker for natural language processing in vaccine safety. BMC medical informatics and decision making, 7, 3. doi:10.1186/1472-6947-7-3&lt;/p&gt;

&lt;p&gt;Wang, S., &amp;amp; Manning, C.D. (2012). Baselines and bigrams: Simple, good sentiment and topic classification. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (pp. 90–94). Retrieved from https://www.aclweb.org/anthology/P12-2018&lt;/p&gt;</content><author><name>Anthony Lipphardt</name></author><summary type="html">Background</summary></entry><entry><title type="html">Word Embeddings Demo</title><link href="http://localhost:4000/2019/03/20/Word-Embeddings.html" rel="alternate" type="text/html" title="Word Embeddings Demo" /><published>2019-03-20T00:00:00-04:00</published><updated>2019-03-20T00:00:00-04:00</updated><id>http://localhost:4000/2019/03/20/Word-Embeddings</id><content type="html" xml:base="http://localhost:4000/2019/03/20/Word-Embeddings.html">&lt;p&gt;&lt;a href=&quot;https://rpubs.com/alipphardt/478255&quot;&gt;RPubs - Word Embeddings Demo&lt;/a&gt;
&lt;a href=&quot;https://github.com/alipphardt/visualize_embeddings&quot;&gt;GitHub Repo - Word2Vec Embedding Projector in Tensorflow&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This R Markdown report, published to RPubs, demonstrates the use of several machine learning and natural language processing techniques surrounding the concept of word embeddings.&lt;/p&gt;

&lt;p&gt;Word embeddings are a form of language modeling using natural language processing and machine learning techniques to map words from a selected vocabulary to a vector of numbers. For example:&lt;/p&gt;

&lt;p&gt;“DRUG” = [-3.2030731e-02, 2.7105689e-01, -4.9149340e-01, 6.1396444e-01, … ]&lt;/p&gt;

&lt;p&gt;By representing words with vectors, we can map words to a point in a high dimensional geometric space. The goal of word embedding models are to place words in this geometric space such that words with high semantic similarity are close together. How these vectors are constructed depend on the models or algorithms used. Word2Vec and FastText algorithms are the focus of this demo.&lt;/p&gt;

&lt;h2 id=&quot;sample-analyses&quot;&gt;Sample Analyses&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Examining the quality of embeddings produced by Word2Vec and FastText algorithms&lt;/li&gt;
  &lt;li&gt;TSNE dimensionality reduction to visualize high dimensional vectors in 2D&lt;/li&gt;
  &lt;li&gt;Exploring Word2Vec embeddings using &lt;a href=&quot;https://projector.tensorflow.org/?config=https://gist.githubusercontent.com/alipphardt/9341654aa777b9bf803de0c6e04b0ec5/raw/85ad4bb537b7dfa7116d07e3f4617d2639a654aa/projector-config.json&quot;&gt;Tensorflow Embedding Projector&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/images/word-embeddings/word-embedding-sample.png&quot; alt=&quot;Word Embedding Sample&quot; /&gt;
&lt;img src=&quot;/images/word-embeddings/TSNE.png&quot; alt=&quot;TSNE dimensionality reduction&quot; /&gt;
&lt;img src=&quot;/images/word-embeddings/tensorflow-model.png&quot; alt=&quot;Embedding Projector in Tensorflow&quot; /&gt;&lt;/p&gt;</content><author><name>Anthony Lipphardt</name></author><summary type="html">RPubs - Word Embeddings Demo GitHub Repo - Word2Vec Embedding Projector in Tensorflow</summary></entry><entry><title type="html">Analysis of Literal Text from the Death Certificate</title><link href="http://localhost:4000/2019/02/20/Literal-Text-Analysis.html" rel="alternate" type="text/html" title="Analysis of Literal Text from the Death Certificate" /><published>2019-02-20T00:00:00-05:00</published><updated>2019-02-20T00:00:00-05:00</updated><id>http://localhost:4000/2019/02/20/Literal-Text-Analysis</id><content type="html" xml:base="http://localhost:4000/2019/02/20/Literal-Text-Analysis.html">&lt;p&gt;&lt;a href=&quot;https://rpubs.com/alipphardt/469311&quot;&gt;RPubs - Analysis of Literal Text from the Death Certificate&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This R Markdown report, published to RPubs, performs text mining analysis on a representative sample of death records from public data available for purchase from the Washington State Department of Health. The purpose of this project is to perform exploratory analyses prior to a larger examination of records for the United States from 2003-2017. The analysis will focus primarily on records pertaining to drug overdose deaths with the following ICD-10 codes: X40-44, X60-64, X85, Y10-Y14.&lt;/p&gt;

&lt;h2 id=&quot;sample-analyses&quot;&gt;Sample Analyses&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Top words among all death records.&lt;/li&gt;
  &lt;li&gt;Top words according to ICD code and drug overdose status, examining raw counts and term frequency inverse document frequency (TF-IDF).&lt;/li&gt;
  &lt;li&gt;Top bigrams, including network graph visualizations depicting the relationship of common word pairs.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&quot;https://rpubs.com/alipphardt/469311&quot;&gt;Refer to RPubs Report for Analyses&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/literal-text-analyses/top-words.png&quot; alt=&quot;Top Words&quot; /&gt;
&lt;img src=&quot;/images/literal-text-analyses/top-words-by-overdose-status.png&quot; alt=&quot;Top Words by overdose status&quot; /&gt;
&lt;img src=&quot;/images/literal-text-analyses/top-words-by-ICD.png&quot; alt=&quot;Top Words by ICD code&quot; /&gt;
&lt;img src=&quot;/images/literal-text-analyses/bigrams.png&quot; alt=&quot;Top bigrams, represented by network graph&quot; /&gt;&lt;/p&gt;</content><author><name>Anthony Lipphardt</name></author><summary type="html">RPubs - Analysis of Literal Text from the Death Certificate</summary></entry><entry><title type="html">2013 Baltimore Speed Camera Citations</title><link href="http://localhost:4000/2019/01/19/Baltimore-Speed-Camera-Citations.html" rel="alternate" type="text/html" title="2013 Baltimore Speed Camera Citations" /><published>2019-01-19T00:00:00-05:00</published><updated>2019-01-19T00:00:00-05:00</updated><id>http://localhost:4000/2019/01/19/Baltimore-Speed-Camera-Citations</id><content type="html" xml:base="http://localhost:4000/2019/01/19/Baltimore-Speed-Camera-Citations.html">&lt;p&gt;&lt;a href=&quot;https://rpubs.com/alipphardt/466021&quot;&gt;RPubs - 2013 Baltimore Speed Camera Citations&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This R Markdown report, published to RPubs, focuses on data cleaning and exploratory analyses of 2.4 million records for speed camera citations in the City of Baltimore from 2013 to the present.&lt;/p&gt;</content><author><name>Anthony Lipphardt</name></author><summary type="html">RPubs - 2013 Baltimore Speed Camera Citations</summary></entry><entry><title type="html">Face Detection using Open CV</title><link href="http://localhost:4000/2018/12/19/Face-Detection.html" rel="alternate" type="text/html" title="Face Detection using Open CV" /><published>2018-12-19T00:00:00-05:00</published><updated>2018-12-19T00:00:00-05:00</updated><id>http://localhost:4000/2018/12/19/Face-Detection</id><content type="html" xml:base="http://localhost:4000/2018/12/19/Face-Detection.html">&lt;p&gt;&lt;a href=&quot;https://github.com/alipphardt/face-detection&quot;&gt;GitHub Repo&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;choice-of-dataset&quot;&gt;Choice of Dataset&lt;/h2&gt;

&lt;p&gt;Ring Doorbell videos&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Video surveillance in the event of motion detection or doorbell ring&lt;/li&gt;
  &lt;li&gt;MP4 videos range in length from 15-30 seconds and run at 15 frames per second&lt;/li&gt;
  &lt;li&gt;Daytime and night-vision modes which record in color or grayscale&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/images/face-detection/ring-doorbell-recordings.png&quot; alt=&quot;Ring Doorbell recordings&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;goal&quot;&gt;Goal&lt;/h2&gt;
&lt;p&gt;Individuals are recognized largely by their facial features, therefore it is desirable to detect and isolate faces within a video. Face detection is import for the purposes of video surveillance as the goal is to extract semantic information such as ‘individual on my doorstep at this timestamp’.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/face-detection/face-detection.png&quot; alt=&quot;Example of face detection&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;techniques&quot;&gt;Techniques&lt;/h2&gt;
&lt;p&gt;Techniques from multimedia data mining will be used within the project for preprocessing, classification/detection, and segmentation (extracting objects of interest from an image).&lt;/p&gt;

&lt;p&gt;Given that the model of ring doorbell used in this project had variations in quality and a grainy quality, I used several types of pre-processing to see what would work best to bring features of interest to the forefront. This included the use of Gaussian or median blur for denoising, to reduce the amount of pixelation in the images; and locally adaptive histogram equalization, which adjusts the contrast within regions of an image, which can be important when regions of the video are susceptible to glare or poor lighting.&lt;/p&gt;

&lt;p&gt;Haar feature-based cascade classifiers are pre-trained models available in the Open CV package which may be used to detect faces at various angles (frontal, angled, sideways/profile). This is useful for detecting regions within the image with a potential face and returning a bounding box.&lt;/p&gt;

&lt;p&gt;Once the face is detected, it is desirable to segment/extract the face from the background. This is done using the GrabCut algorithm. GrabCut typically works best when accompanied with interactive methods where rectangles or polygons are roughly drawn around the object of interest, so it is expected that this would work well given a bounding box.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/face-detection/haar.png&quot; alt=&quot;Example of HAAR classifier&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;high-level-algorithm&quot;&gt;High Level Algorithm&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Create VideoCapture object and read in keyframes from source video&lt;/li&gt;
  &lt;li&gt;For each keyframe:
    &lt;ul&gt;
      &lt;li&gt;Convert to grayscale (required for equalization)&lt;/li&gt;
      &lt;li&gt;Apply CLAHE (8x8 tile) to enhance contrast and edges&lt;/li&gt;
      &lt;li&gt;Run face detection using frontal face classifier to detect all faces, returning bounding boxes&lt;/li&gt;
      &lt;li&gt;For each bounding box, perform GrabCut segmentation (initializing with bounding box) to obtain a mask and add masks together to obtain a mask that isolates all faces in frame&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Combine variations of image (i.e. original, preprocessed, face bounding boxes, isolated faces) into frame with 4 panel display&lt;/li&gt;
  &lt;li&gt;Write these frames to a video output file&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;demo&quot;&gt;Demo&lt;/h2&gt;
&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/vsYzCad0C6A&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/wZNDZF_limo&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/_i10s4_4uuQ&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/H--FkanYZbU&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/aiYb_fuDMWQ&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;h2 id=&quot;limitationsconsiderations&quot;&gt;Limitations/Considerations&lt;/h2&gt;
&lt;p&gt;This method was tested against a number of ring doorbell videos using variation in lighting and movement. Generally, this method works well when lighting conditions are favorable, movement is relatively slow, and the face is unobstructed.&lt;/p&gt;

&lt;p&gt;The current model of the Ring Doorbell has large variations in video quality depending on whether the camera was running in daytime or nightime modes. The videos in this model were relatively poor clarity compared to high resolution cameras, and there were high amounts of noise that proved troublesome for pre-processing.&lt;/p&gt;

&lt;p&gt;I found that the placement of the doorbell also made a significant difference with respect to the HAAR classifiers. They generally worked well when taking the face from straight on. However, with our current home, individuals approach the camera initially from lower ground down a set of steps and once they reach the door are over the camera at a significant angle. There is generally a very short window of time where the faces are at an optimal distance and angle. Changing the height or angle in which the doorbell is mounted could improve performance.&lt;/p&gt;

&lt;p&gt;Additionally, the HAAR classifiers chosen provided mixed results with some false positives appearing. For example, the test image with multiple faces picked up not only the faces of the two individuals, but the women’s neck. Additionally, in most cases faces in fair lighting and good visibility were detected, but there were a number of false positives that were detected potentially as a result of poor picture quality or artifacts. To reduce/remove these anomalies, I would recommend doing some temporal analyses to ensure that faces occur across a minimum set of frames before truly being detected.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/face-detection/two-people.jpg&quot; alt=&quot;two people&quot; /&gt;
&lt;img src=&quot;/images/face-detection/false-positive.png&quot; alt=&quot;false positive&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;future-work&quot;&gt;Future Work&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Additional preprocessing steps to account for disparity in lighting, movement, or angle&lt;/li&gt;
  &lt;li&gt;Reducing false positives using a combination of classifiers (e.g. a frontal face should contain 2 eyes, a face is part of or adjacent to a body)&lt;/li&gt;
  &lt;li&gt;Feature matching extracted faces to individual headshots&lt;/li&gt;
  &lt;li&gt;Produce data on individuals identified at given timestamps that can be queried&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Anthony Lipphardt</name></author><summary type="html">GitHub Repo</summary></entry><entry><title type="html">Classifying Drug Mention with Involvement (DMI) Deaths</title><link href="http://localhost:4000/2018/06/11/DMI-Deaths.html" rel="alternate" type="text/html" title="Classifying Drug Mention with Involvement (DMI) Deaths" /><published>2018-06-11T00:00:00-04:00</published><updated>2018-06-11T00:00:00-04:00</updated><id>http://localhost:4000/2018/06/11/DMI-Deaths</id><content type="html" xml:base="http://localhost:4000/2018/06/11/DMI-Deaths.html">&lt;p&gt;&lt;a href=&quot;https://github.com/alipphardt/dmi-deaths-classification&quot;&gt;GitHub Repo&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/alipphardt/dmi-deaths-classification/blob/master/dmi-classification-presentation.pdf&quot;&gt;Full PDF presentation&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;project-data-and-general-approach&quot;&gt;Project, Data, and General Approach&lt;/h2&gt;

&lt;h3 id=&quot;problem&quot;&gt;Problem&lt;/h3&gt;
&lt;p&gt;The Centers for Disease Control and Prevention’s (CDC) National Center for Health Statistics (NCHS) works in partnership with state and local governments to collect, analyze, and disseminate data on vital statistics, including birth and death events. This data is used to monitor trends of public health importance such as the ongoing opioid crisis. The most current data from 2016 indicates that deaths from opioids were 5 times higher than in 1999, and provisional estimates indicate that these numbers are continuing to rise.&lt;/p&gt;

&lt;p&gt;Mortality data is currently coded using the International Classification of Diseases, Tenth Revision (ICD-10), which is used to identify underlying and contributory causes of death. Coding for drug involved deaths are currently problematic for a number of reasons including:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Initial coding of ICD-10 classifications&lt;/strong&gt; -  NCHS uses automated programs to assign 1 underlying cause of death and up to 20 multiple causes of death. In general, the program rejects about one fifth of death records which must be reviewed and coded by trained nosologists. However, for deaths with an underlying cause of drug overdose, about two-thirds of records are manually coded.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Limited codes for specific drugs&lt;/strong&gt; - ICD-10 is limited in capturing drug-involved mortality as it only contains a few codes pertaining to specific drugs (e.g. heroin, methadone, cocaine) and these codes are only applied in specific circumstances. Most drugs are coded using broad categories, making it difficult to monitor trends in specific drugs that are not already uniquely classified by ICD-10.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Recent efforts have been placed on developing programs to extract drug mentions from literal text fields of the U.S. Standard Certificate of Death. These fields include:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The chain of events leading to death (from Part I)&lt;/li&gt;
  &lt;li&gt;Other siginificant conditions that contributed to cause of death (from Part II)&lt;/li&gt;
  &lt;li&gt;How the injury occurred (in the case of deaths due to injuries [from Box 43])&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Programs developed rely on exact pattern matching and significant collaboration between NCHS and the Food and Drug Administration (FDA) to develop search terms for identified drug mentions, descriptors (e.g., illicit, prescription/RX), and contextual phrases (e.g., Ingested, History/HX of Abuse). Use of literal text analysis provides an opportunity for an enhanced understanding of the national picture of drug involvement in deaths in the United States, but is also problematic in the time involved to develop and maintain programs based on exact pattern matching.&lt;/p&gt;

&lt;h3 id=&quot;data&quot;&gt;Data&lt;/h3&gt;
&lt;p&gt;NCHS performs literal text analysis using final death files from NCHS’ National Vital Statistics System linked to literal text data. This data is collected from state and local vital records offices and cleaned and coded in preparation for analysis. Due to privacy and confidentiality concerns, literal text data files are unavailable outside of the CDC/NCHS secure data platform.&lt;/p&gt;

&lt;p&gt;However, &lt;a href=&quot;https://www.doh.wa.gov/DataandStatisticalReports/HealthStatistics/Death&quot;&gt;Washington State’s Department of Health&lt;/a&gt; provides both annual files and literal text files for purchase, which are largely similar to the information collected by NCHS.&lt;/p&gt;

&lt;p&gt;For the purpose of this analysis, the most recent 2 years of data (i.e., 2015 and 2016 annual files and literal text files) were purchased for use. Each dataset was provided as a CSV dataset and has been linked through common attributes such as year and death certificate number. Each dataset includes records for approximately 50,000 deaths in the State of Washington.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Original annual and literal text files have been excluded from the GitHub repo. However, preprocessed data with the combined literal text fields are made available as a CSV.&lt;/p&gt;

&lt;p&gt;The annual files, have approximately 133 fields, which are described by the &lt;a href=&quot;https://www.doh.wa.gov/Portals/1/Documents/5300/DeathStatisticalFileLayout.xlsx&quot;&gt;File Layout [XLSX]&lt;/a&gt; on the Washington Department of Health website.&lt;/p&gt;

&lt;p&gt;Of interest in the annual file is the certificate number, used for linking, and the ICD-10 code for underlying cause of death and up to 20 multiple causes of death.&lt;/p&gt;

&lt;p&gt;The literal text files, have the following layout:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;‘State File Number’,&lt;/li&gt;
  &lt;li&gt;‘Cause of Death - Line A’&lt;/li&gt;
  &lt;li&gt;‘Cause of Death - Line B’&lt;/li&gt;
  &lt;li&gt;‘Cause of Death - Line C’,&lt;/li&gt;
  &lt;li&gt;‘Cause of Death - Line D’&lt;/li&gt;
  &lt;li&gt;‘Interval Time - Line A’,&lt;/li&gt;
  &lt;li&gt;‘Interval Time - Line B’&lt;/li&gt;
  &lt;li&gt;‘Interval Time - Line C’,&lt;/li&gt;
  &lt;li&gt;‘Interval Time - Line D’&lt;/li&gt;
  &lt;li&gt;‘Other Significant Conditions’,&lt;/li&gt;
  &lt;li&gt;‘How Injury Occurred’&lt;/li&gt;
  &lt;li&gt;‘Place of Injury’&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;From the linked file, a derived feature will be created as a label to classify a record as a drug mentioned with involvement (DMI) death or not. Death records coded with an underlying cause of death ICD code (i.e., X40–X44, X60–X64, X85, or Y10–Y14) will be marked as a DMI death. Additionally, remaining records with flagged drug mentions will be reviewed manually and classified as a DMI death.&lt;/p&gt;

&lt;p&gt;Preprocessed data includes CSV with literal text field and DMI target and is named: &lt;strong&gt;literal-text.csv&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The final dataset created within the experiments will consist of a matrix of binary term-frequencies and a class label called DMI. The full term-frequency matrix is not stored due to large file size (12 GB) depending on hyper-parameters used by CountVectorizer.&lt;/p&gt;

&lt;h3 id=&quot;objectives&quot;&gt;Objectives&lt;/h3&gt;
&lt;p&gt;This project will address whether machine learning techniques can be used to train a classifier to predict with high precision and specificity whether a death record is a drug mention with involvement (DMI) death or not.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Precision, also known as positive predictive value, is the proportion of records predicted to be in the positive class (in this case a DMI death) that were correctly classified (TP/TP+FP).&lt;/li&gt;
  &lt;li&gt;Specificity, also known as true negative rate, is the proportion of records in the negative class that were accurately predicted (TN/TN+FP).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We wish to maximize these two metrics as the problem demands that we find a solution that accurately determines whether a record is in the positive class (DMI death) or not and reduces or avoids instances of false positives. Current programs within NCHS that rely on exact pattern matching achieve a precision score of 95.8 percent on the DMI class. The objective of this project is to find a solution that exceeds this baseline.&lt;/p&gt;

&lt;p&gt;The linked dataset will be converted to a term-frequency matrix that stores each word/term as a 1 if the term is used in a given record and 0 if it is not. As discussed previously, a derived feature will be created using existing ICD codes and flagged drug mentions to identify whether a record is a DMI death or not.&lt;/p&gt;

&lt;p&gt;The intended benefits of the described solution are as follows:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Improve identification of DMI deaths, potentially reducing the amount of manual reviews for drug overdose deaths.&lt;/li&gt;
  &lt;li&gt;Develop a generalized solution that removes the need for development of exhaustive lists of search terms, descriptors, and contextual phrases.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Once a classifier is obtained to more accurately identify DMI deaths, we can extract drug mentions from those records to better track trends of drug involvement in deaths.&lt;/p&gt;

&lt;h3 id=&quot;approach&quot;&gt;Approach&lt;/h3&gt;
&lt;p&gt;Our approach will use the following machine learning techniques:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Bag-of-words/Term Frequencies Model (&lt;a href=&quot;http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html&quot;&gt;CountVectorizer in scikit-learn&lt;/a&gt;)&lt;/strong&gt; - The dataset will be transformed into a matrix of binary term-frequencies (0 or 1), which is common in text mining tasks. Words that appear more than 3 times will be included and combinations of individual words and adjacent word pairings will be tested.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Support Vector Machine (&lt;a href=&quot;http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html&quot;&gt;SVC in scikit-learn&lt;/a&gt;)&lt;/strong&gt; - This algorithm is popular in text classification tasks and is suitable for datasets that are not too large. Since our task is a binary classification, our solution only requires a single SVM. Our dataset consists of 100,000 records, but it is expected to be a fraction of that once we obtain our class labels and balance the dataset. SVMs are popular in text classification as they scale well for high dimensionality data and can perform with high accuracy and stability on both linear and nonlinear problems.
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Latent Semantic Analysis (&lt;a href=&quot;http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html&quot;&gt;TruncatedSVD in scikit-learn&lt;/a&gt;)&lt;/strong&gt; - Given that the creation of a term-frequency matrix will likely result in thousands of features, dimensionality reduction may prove useful in improving training speed and predictive power of the classifier, particularly if pairs of terms are heavily correlated. Note that we are only applying this to SVM as Naive Bayes expects discrete data.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Naive Bayes (&lt;a href=&quot;http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html&quot;&gt;Bernoulli Naive Bayes in scikit-learn&lt;/a&gt;)&lt;/strong&gt;- Despite the assumption of independence, Naive Bayes has been found to work well in text classification. It works well for this scenario as the algorithm is quick to train and scales well on high dimensionality data. Because our term-frequency matrix will only consist of 0s and 1s, we have discrete values, which Naive Bayes works well on.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;pre-processing&quot;&gt;Pre-processing&lt;/h2&gt;

&lt;h3 id=&quot;data-cleaning&quot;&gt;Data Cleaning&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;2015 and 2016 documents used different names for columns. Renaming was done to conform to standard column labels. This would facilitate concatenation and merging of the datasets.&lt;/li&gt;
  &lt;li&gt;The following fields were retained:
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;State file number&lt;/strong&gt; (i.e., certificate number)&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Underlying Cause-of-Death&lt;/strong&gt; (single cause)&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Multiple Cause-of-Death&lt;/strong&gt; (up to 20 causes)&lt;/li&gt;
      &lt;li&gt;The chain of events leading to death (from Part I)
        &lt;ul&gt;
          &lt;li&gt;&lt;strong&gt;Cause-of-Death Line A&lt;/strong&gt;&lt;/li&gt;
          &lt;li&gt;&lt;strong&gt;Cause-of-Death Line B&lt;/strong&gt;&lt;/li&gt;
          &lt;li&gt;&lt;strong&gt;Cause-of-Death Line C&lt;/strong&gt;&lt;/li&gt;
          &lt;li&gt;&lt;strong&gt;Cause-of-Death Line D&lt;/strong&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Other siginificant conditions that contributed to cause of death (from Part II)
        &lt;ul&gt;
          &lt;li&gt;&lt;strong&gt;Other Significant Conditions&lt;/strong&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;How the injury occurred (in the case of deaths due to injuries [from Box 43])
        &lt;ul&gt;
          &lt;li&gt;&lt;strong&gt;How the Injury Occurred&lt;/strong&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Irrelevant fields were dropped from the dataset. These include but are not limited to:
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Interval Time - Line (A/B/C/D)&lt;/strong&gt;&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;COD-DUE-TO-(B/C/D)&lt;/strong&gt; - Further inspection shows only one record in 2015 using this field&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Injury Place&lt;/strong&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;A left join between the annual and literal text files was performed to create a single dataset with DMI targets and literal text fields. Literal text fields including Part I cause of death variables, How Injury Occurred, and Other Significant Conditions were combined into a single literal text field.&lt;/li&gt;
  &lt;li&gt;Rows with missing values for the literal text and DMI fields were removed.&lt;/li&gt;
  &lt;li&gt;Literal text field was stripped of punctuation and stop words using NLTK module, as well as removing numbers.&lt;/li&gt;
  &lt;li&gt;Standardization was not performed as term-frequency matrix will consist of discrete 0’s or 1’s in order to use classifiers such as Naive Bayes.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;feature-engineering&quot;&gt;Feature Engineering&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Year&lt;/strong&gt; field was added to 2015-2016 annual and literal text files for the purpose of linkage between these datasets&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;DMI&lt;/strong&gt; field was added as a target label. Deaths with an underlying or multiple cause of death for the following ICD codes were flagged as a DMI death: X40–X44, X60–X64, X85, or Y10–Y14. Following pre-processing, the following counts for DMI were obtained:
    &lt;ul&gt;
      &lt;li&gt;DMI (1) - 2498&lt;/li&gt;
      &lt;li&gt;Not DMI (0) - 109407&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;The literal-text field was converted to a &lt;strong&gt;term-frequency matrix&lt;/strong&gt; using scikit-learn’s &lt;a href=&quot;http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html&quot;&gt;CountVectorizer&lt;/a&gt; module. Model was created specifying binary frequencies (0 or 1) and a minimum frequency of 5 in the dataset to select a feature. This term frequency matrix was pickled and written out to a file for the next phase of the project, which will combine into a a single data frame with the DMI targets. The python notebook &lt;strong&gt;preprocessing.ipynb&lt;/strong&gt; includes an example of this transformation. This data frame itself was too large to pickle/write to a file (11 GB).&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;experimental-plan&quot;&gt;Experimental Plan&lt;/h2&gt;

&lt;p&gt;Four machine learning techniques from scikit-learn will be utilized in this experiment: CountVectorizer, Latent Semantic Analysis, Naïve Bayes, and Support Vector Machine.&lt;/p&gt;

&lt;p&gt;The first step will be to fit the literal text dataset using CountVectorizer to convert the single literal text field into a bag-of-words or term frequencies model with binary frequencies. The results of this dataset will then be included in one of three pipelines:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Fit to a Naïve Bayes classifier.&lt;/li&gt;
  &lt;li&gt;Fit to a Support Vector Machine (SVM) classifier without dimensionality reduction.&lt;/li&gt;
  &lt;li&gt;Perform dimensionality reduction using Latent Semantic Analysis (LSA) and fit to a SVM classifier.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Note: PCA was originally planned as the means for dimensionality reduction in this project. However, scikit-learn’s implementation cannot work with the sparse datasets produced by CountVectorizer. Therefore, LSA was used in a similar manner to reduce dimensions while retaining 85-95 percent of explained variance in the dataset.&lt;/p&gt;

&lt;p&gt;The workflow will be managed using scikit-learn’s grid search and pipeline constructs. Grid search allows you to set up a parameter grid for each machine learning technique you plan to use and will test every combination within the parameter grid to find an optimal configuration based on your chosen scoring metric. In this experiment, we will use 10-fold cross validation using precision as our scoring metric in the grid search. The pipeline construct allows you to chain together multiple machine learning techniques in scikit-learn while performing the grid search.&lt;/p&gt;

&lt;p&gt;Below we detail the combination of parameters that will be supplied to the parameter grids in the initial grid search. Additional parameters may be added as needed.&lt;/p&gt;

&lt;p&gt;Based on three pipelines, we will have 35 combinations of parameters: 16 combinations for Naïve Bayes, 16 for SVM, and 3 for SVM without dimensionality. Grid search will perform 10-fold cross validation and select the best parameters based on average precision score.&lt;/p&gt;

&lt;p&gt;Prior to running the grid search, the data will be balanced, under-sampling the over-represented class (Non-DMI) to balance the two classes. This is particularly important as the class ratio before under-sampling is roughly 50 to 1. The dataset will then be split into training and test sets using a 75:25 split given the relatively small dataset size once balanced. The portion of the data set aside for training will be passed to the grid search, while the remaining data will be used for final testing and scored on precision and specificity for the DMI class.&lt;/p&gt;

&lt;p&gt;The process for these experiments are shown in the provided &lt;strong&gt;experiments.ipynb&lt;/strong&gt; Jupyter notebook. Results of the grid search are in the &lt;strong&gt;results&lt;/strong&gt; folder as a CSV file.&lt;/p&gt;

&lt;h3 id=&quot;ml-technique-1-bag-of-wordsterm-frequencies-model-countvectorizer-in-scikit-learn&quot;&gt;ML Technique #1: Bag-of-Words/Term Frequencies Model (&lt;a href=&quot;http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html&quot;&gt;CountVectorizer in scikit-learn&lt;/a&gt;)&lt;/h3&gt;

&lt;p&gt;The bag-of-words model is a common representation for text classification tasks which represents each record as a bag or collection of words - pulled from all records/documents in the dataset - and their frequencies within that particular record. No consideration is given to order, and so the model may be extended to include N-grams which contain groups of N adjacent words.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Combination&lt;/th&gt;
      &lt;th&gt;lowercase&lt;/th&gt;
      &lt;th&gt;binary&lt;/th&gt;
      &lt;th&gt;min_df&lt;/th&gt;
      &lt;th&gt;ngram_range&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;(1,1)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;(1,2)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;(1,1)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;(1,2)&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Each combination of parameters will disable conversion to lower-case, since the dataset is already converted to all uppercase, and binary will be set to True, which will set term frequencies to either 0 or 1, giving us the flexibility to use popular ML techniques such as Naive Bayes which requires discrete data.&lt;/p&gt;

&lt;p&gt;The min_df parameter specifies the minimum number of times a word must appear within the dataset for it to be included in the vocabulary for the bag of words model. Three and five are chosen to rule out the potential for typos and misspellings.&lt;/p&gt;

&lt;p&gt;N-gram range will be set to (1,1) or (1,2) to include individual words and adjacent word pairs in the bag of words model.&lt;/p&gt;

&lt;h3 id=&quot;ml-technique-2-latent-semantic-analysis-truncatedsvd-in-scikit-learn&quot;&gt;ML Technique #2: Latent Semantic Analysis (&lt;a href=&quot;http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html&quot;&gt;TruncatedSVD in scikit-learn&lt;/a&gt;)&lt;/h3&gt;

&lt;p&gt;Dimensionality reduction is a useful technique that can assist ML algorithms in more quickly converging on a solution by combining variables into n component features that each combine related features. LSA is similar to PCA in that it applies singular value decomposition to find a transformation that rotates axes in a manner that projects the data to a lower dimensional feature space. The main difference is that LSA accepts a sparse term-frequency matrix – obtained here through CountVectorizer – and obtains an approximation by computing three matrices: a term-concept matrix, a singular values matrix with eigenvalues on the diagonal, and a concept-document matrix. These three matrices produce an approximation of the original term-frequencies matrix.&lt;/p&gt;

&lt;p&gt;As discussed in the approach section of the readme file, dimensionality reduction will only be applied to the SVM classifier as Naive Bayes requires discrete data.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Combination&lt;/th&gt;
      &lt;th&gt;n_components&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;525&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;775&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;1180&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;With scikit-learn’s PCA implementation, we could specify a floating-point number for n_components to indicate what percentage of explained variance we would like to retain in the dataset with our chosen components. However, scikit-learn’s implementation of LSA only accepts an integer for the n_components parameter.&lt;/p&gt;

&lt;p&gt;Since our value for the n_components parameter is dependent on the number of words in our dataset – which relies on the choice of ngram_range for CountVectorizer – we will determine optimal parameters for our experiments after running an initial grid search for the first two pipelines. Using optimal parameters, we will then fit a model using CountVectorizer and TruncatedSVD and determine number of components that retain approximately 85, 90, and 95 percent of explained variance in the dataset. The values in the above table reflect this approach.&lt;/p&gt;

&lt;h3 id=&quot;ml-technique-3-support-vector-machine-svc-in-scikit-learn&quot;&gt;ML Technique #3: Support Vector Machine (&lt;a href=&quot;http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html&quot;&gt;SVC in scikit-learn&lt;/a&gt;)&lt;/h3&gt;

&lt;p&gt;The Support Vector Machine is one of two classifiers that will be used in this experiment. SVM have been known to do well in applications for text classification with high dimensionality where the dataset is not too large. For this application, a balanced training set will contain approximately 5,000 records, which is suitable for SVM.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Combination&lt;/th&gt;
      &lt;th&gt;kernel&lt;/th&gt;
      &lt;th&gt;C&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;Linear&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;Linear&lt;/td&gt;
      &lt;td&gt;10&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;Linear&lt;/td&gt;
      &lt;td&gt;100&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;Linear&lt;/td&gt;
      &lt;td&gt;1000&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The main choice in configuring SVM is the choice of kernel, followed by the appropriate hyperparameters asscoaited with that kernel.&lt;/p&gt;

&lt;p&gt;A linear kernel will be used in this experiment as many applications of text classification can be linearly separable, particularly as a result of the high dimensionality and large dataset sizes for text classification datasets. Linear classifiers are also much faster to train and only require tuning of the regularization parameter C. For this experiment we will test values of 1, 10, 100, and 1000 for the C parameter.&lt;/p&gt;

&lt;p&gt;If it is found that the SVM does poorly with a linear kernel, additional experiments will be conducted with other types of kernel to see if performance improves.&lt;/p&gt;

&lt;h3 id=&quot;ml-technique-4-naive-bayes-bernoulli-naive-bayes-in-scikit-learn&quot;&gt;ML Technique #4: Naive Bayes (&lt;a href=&quot;http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html&quot;&gt;Bernoulli Naive Bayes in scikit-learn&lt;/a&gt;)&lt;/h3&gt;

&lt;p&gt;Since our bag of words model uses binary frequencies of 0 or 1, we use the Bernoulli Naive Bayes classifier in scikit-learn, which is optimized for binary counts.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Combination&lt;/th&gt;
      &lt;th&gt;binarize&lt;/th&gt;
      &lt;th&gt;alpha&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;None&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;None&lt;/td&gt;
      &lt;td&gt;0.1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;None&lt;/td&gt;
      &lt;td&gt;0.5&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;None&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The binarize parameter is set to None in all instances to disable the conversion to binary counts since this has already been performed by CountVectorizer.&lt;/p&gt;

&lt;p&gt;The alpha parameter controls the level of smoothing applied in the training set. This can be useful when items in the test set would have zero probability based on the training set. There is no good rule of thumb for setting this parameter, so the experiment will include several values within a parameter Grid Search.&lt;/p&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;

&lt;h3 id=&quot;naïve-bayes&quot;&gt;Naïve Bayes&lt;/h3&gt;

&lt;p&gt;The alpha parameter is the main hyper-parameter adjusted in Naïve Bayes and controls the level of smoothing applied to the model, accounting for features in the test set not encountered in the training set and would otherwise compute a probability of zero.&lt;/p&gt;

&lt;p&gt;The below graph shows mean precision score over 10-fold cross validation for each value of alpha tested and is grouped by values for min_df and ngram_range used in CountVectorizer. We can see that an alpha value of 1 (the default Laplace smoothing using in scikit-learn) results in the best average precision score and that in general lower values for alpha (Lidstone smoothing) result in lower average precision scores. In general, we can also see that for Naïve Bayes that including bi-grams (ngram_range = (1,2)) generally performs better than the alternative of singles words in the term-frequencies matrix.&lt;/p&gt;

&lt;p&gt;The grid search object returned by scikit-learn also returns basic statistics on average time to fit and score each combination of parameters. Naïve Bayes is popular as it is relatively fast to both train and make predictions. For this particular dataset, there was an average fit time of 0.0987 seconds and an average scoring time of 0.0104 seconds.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/dmi-deaths/nb-results.png&quot; alt=&quot;Naive Bayes Results&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;support-vector-machine-without-dimensionality-reduction&quot;&gt;Support Vector Machine without Dimensionality Reduction&lt;/h3&gt;

&lt;p&gt;Using a linear kernel, the C parameter is the main hyper-parameter adjusted for the Support Vector Machine and controls the error penalty used when misclassifying an example. Larger values of C generally mean a smaller margin around the separating hyperplane.&lt;/p&gt;

&lt;p&gt;The below graph shows mean precision score over 10-fold cross validation for each value of C tested and is grouped by values for min_df and ngram_range used in CountVectorizer. We can see that for this particular dataset a smaller value of C generally results in a better classification score, particularly when min_df is set to 5. For min_df of 3 there is a very small difference. In general, we can also see that including bi-grams (ngram_range = (1,2)) generally performs
better than the alternative of singles words in the term-frequencies matrix.&lt;/p&gt;

&lt;p&gt;Support Vector Machines can be problematic in that they are slow to train for larger datasets. However, for this particular dataset, it worked fairly well with an average fit time of 0.2176 seconds and an average scoring time of 0.0190 seconds.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/dmi-deaths/svm1-results.png&quot; alt=&quot;SVM Results&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;support-vector-machine-and-latent-semantic-analysis&quot;&gt;Support Vector Machine and Latent Semantic Analysis&lt;/h3&gt;

&lt;p&gt;As discussed in the Experimental Plan, the n_components parameter for TruncatedSVD/LSA is dependent on the number of features in the dataset and is therefore dependent on the parameters set in CountVectorizer. After running grid search against Naïve Bayes and SVM, we selected chose the best performing estimator and parameters, which was SVM with C=1 and CountVectorizer using min_df=3 and ngram_range=(1,2). Using these parameters, the training model was fit using a variety of values to determine the approximate number of components that would result in 85, 90, and 95 percent of explained variance given the dataset. This resulted in the chosen values 525, 775, and 1180.&lt;/p&gt;

&lt;p&gt;The below graph shows mean precision score over 10-fold cross validation for each value of n_components tested with LSA and SVM. We can see that in general the larger values of n_components that retains explained variance results in better precision scores. Given this finding it will be interesting to see whether LSA provides better performance than when all
terms are retained in the dataset.&lt;/p&gt;

&lt;p&gt;With LSA added to the third pipeline for grid search, it computed an average fit time of 5.67 seconds and an average scoring time of 0.1321 seconds, significantly longer than Naïve Bayes and SVM without dimensionality reduction.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/dmi-deaths/svm2-results.png&quot; alt=&quot;SVM with LSA Results&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;comparison-of-three-options&quot;&gt;Comparison of Three Options&lt;/h3&gt;

&lt;p&gt;The below chart includes a side-by-side comparison of the average precision scores for 10-fold cross validation using optimal parameters for each classifier option. Both Naïve Bayes and SVM options work particularly well for this application, with average precision above 98 percent. The top performing classifier was SVM in conjunction with LSA retaining 95 percent of explained variance.&lt;/p&gt;

&lt;p&gt;As discussed in prior sections, SVM can be problematic for larger datasets and when adding LSA we saw an additional 20 times increase in computing time. This is worth considering for training sets that surpass millions of records. However, in this scenario we will choose the SVM with LSA option in scoring against our test dataset.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/dmi-deaths/comparison.png&quot; alt=&quot;Model Comparison&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;performance-with-test-data&quot;&gt;Performance with Test Data&lt;/h3&gt;

&lt;p&gt;Using optimal parameters for CountVectorizer (min_df=3, ngram_range=(1,2)), TruncatedSVD/LSA (n_components=1180), and SVM (kernel=linear, C=1), models were fit to the training data and then test data was transformed.&lt;/p&gt;

&lt;p&gt;Predicted class labels were obtained for each transformed row in the test set and a classification report was produced in scikit-learn (below) which provides precision, recall, and F1 scores for each class label.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt; &lt;/th&gt;
      &lt;th&gt;Precision&lt;/th&gt;
      &lt;th&gt;Recall&lt;/th&gt;
      &lt;th&gt;F1 Score&lt;/th&gt;
      &lt;th&gt;Support&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Non-DMI&lt;/td&gt;
      &lt;td&gt;0.98&lt;/td&gt;
      &lt;td&gt;0.99&lt;/td&gt;
      &lt;td&gt;0.98&lt;/td&gt;
      &lt;td&gt;621&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;DMI&lt;/td&gt;
      &lt;td&gt;0.99&lt;/td&gt;
      &lt;td&gt;0.98&lt;/td&gt;
      &lt;td&gt;0.98&lt;/td&gt;
      &lt;td&gt;627&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Avg/Total&lt;/td&gt;
      &lt;td&gt;0.98&lt;/td&gt;
      &lt;td&gt;0.98&lt;/td&gt;
      &lt;td&gt;0.98&lt;/td&gt;
      &lt;td&gt;1248&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The original problem statement noted that we were particularly interested in maximizing precision and specificity for the DMI class in order to correctly classify DMI deaths while avoiding false positives. The results for our test data show that we have 99 percent precision and specificity for the DMI class (Note: for binary classification, specificity is the same as recall for the negative class).&lt;/p&gt;

&lt;p&gt;A confusion matrix was also produced for the test data (below) and shows that for the Non-DMI class only 8 records were misclassified and for DMI only 15 were misclassified.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Actual\Predicted&lt;/th&gt;
      &lt;th&gt;Non-DMI&lt;/th&gt;
      &lt;th&gt;DMI&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Non-DMI&lt;/td&gt;
      &lt;td&gt;613&lt;/td&gt;
      &lt;td&gt;8&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;DMI&lt;/td&gt;
      &lt;td&gt;15&lt;/td&gt;
      &lt;td&gt;612&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h2&gt;

&lt;p&gt;In this project, we linked annual death statistical files and cause of death literal text files from the State of Washington and used this data to train Naïve Bayes and Support Vector Machine classifiers to distinguish with high precision and specificity whether a given record was a drug mention with involvement death (DMI) or not. Natural language processing techniques were used to remove stop words and punctuation, as well as to convert the linked data to a matrix of term frequencies using unigrams and bigrams. The optimal classifier, Support Vector Machine using Latent Semantic Analysis for dimensionality reduction, achieved precision and specificity of approximately 99 percent for the DMI class on test data. The model exceeded expectations set by exact pattern matching programs, which identified DMI deaths with 95.8 percent precision.&lt;/p&gt;

&lt;p&gt;The success in this experiment is largely attributed to the amount of work in cleaning and preprocessing the data, including the use of natural language processing techniques. With additional cleaning, I believe the model could be further improved. One thought within NCHS is to incorporate systems such as UMLS from the National Institutes of Health, which provide an extensive thesaurus of millions of biomedical concepts and can be used to map synonyms and variations of a term to a principle concept. Reducing the number of terms/features, as we did with LSA in this experiment, improves the ability of the classifier to find an optimal solution.&lt;/p&gt;</content><author><name>Anthony Lipphardt</name></author><summary type="html">GitHub Repo</summary></entry><entry><title type="html">Howard County Police Department Calls for Service, 2014-2016</title><link href="http://localhost:4000/2017/12/17/HCPD-Tableau.html" rel="alternate" type="text/html" title="Howard County Police Department Calls for Service, 2014-2016" /><published>2017-12-17T16:27:59-05:00</published><updated>2017-12-17T16:27:59-05:00</updated><id>http://localhost:4000/2017/12/17/HCPD-Tableau</id><content type="html" xml:base="http://localhost:4000/2017/12/17/HCPD-Tableau.html">&lt;p&gt;This visualization, created with Tableau Public, incorporates data on &lt;a href=&quot;https://opendata.howardcountymd.gov/Public-Safety/Howard-County-Police-Department-Call-For-Service-2/qccx-65fg&quot;&gt;Howard County Police Department Calls for Service (2014-2016)&lt;/a&gt; and started as part of a final exam for my &lt;em&gt;Business Analytics and Strategic Decision Making&lt;/em&gt; class, which asked to create a data visualization in Tableau with a dataset of my choosing.&lt;/p&gt;

&lt;div class=&quot;tableauPlaceholder&quot; id=&quot;viz1513567263140&quot; style=&quot;position: relative&quot;&gt;&lt;noscript&gt;&lt;a href=&quot;#&quot;&gt;&lt;img alt=&quot;Howard County Police Department Calls for Service, 2014-2016 &quot; src=&quot;https:&amp;#47;&amp;#47;public.tableau.com&amp;#47;static&amp;#47;images&amp;#47;Ho&amp;#47;HowardCountyPDCallsforService2014-2016&amp;#47;HowardCountyPoliceDepartmentCallsforService2014-2016&amp;#47;1_rss.png&quot; style=&quot;border: none&quot; /&gt;&lt;/a&gt;&lt;/noscript&gt;&lt;object class=&quot;tableauViz&quot; style=&quot;display:none;&quot;&gt;&lt;param name=&quot;host_url&quot; value=&quot;https%3A%2F%2Fpublic.tableau.com%2F&quot; /&gt; &lt;param name=&quot;embed_code_version&quot; value=&quot;3&quot; /&gt; &lt;param name=&quot;site_root&quot; value=&quot;&quot; /&gt;&lt;param name=&quot;name&quot; value=&quot;HowardCountyPDCallsforService2014-2016&amp;#47;HowardCountyPoliceDepartmentCallsforService2014-2016&quot; /&gt;&lt;param name=&quot;tabs&quot; value=&quot;no&quot; /&gt;&lt;param name=&quot;toolbar&quot; value=&quot;yes&quot; /&gt;&lt;param name=&quot;static_image&quot; value=&quot;https:&amp;#47;&amp;#47;public.tableau.com&amp;#47;static&amp;#47;images&amp;#47;Ho&amp;#47;HowardCountyPDCallsforService2014-2016&amp;#47;HowardCountyPoliceDepartmentCallsforService2014-2016&amp;#47;1.png&quot; /&gt; &lt;param name=&quot;animate_transition&quot; value=&quot;yes&quot; /&gt;&lt;param name=&quot;display_static_image&quot; value=&quot;yes&quot; /&gt;&lt;param name=&quot;display_spinner&quot; value=&quot;yes&quot; /&gt;&lt;param name=&quot;display_overlay&quot; value=&quot;yes&quot; /&gt;&lt;param name=&quot;display_count&quot; value=&quot;yes&quot; /&gt;&lt;param name=&quot;filter&quot; value=&quot;publish=yes&quot; /&gt;&lt;/object&gt;&lt;/div&gt;
&lt;script type=&quot;text/javascript&quot;&gt;                    var divElement = document.getElementById('viz1513567263140');                    var vizElement = divElement.getElementsByTagName('object')[0];                    vizElement.style.width='825px';vizElement.style.height='1027px';                    var scriptElement = document.createElement('script');                    scriptElement.src = 'https://public.tableau.com/javascripts/api/viz_v1.js';                    vizElement.parentNode.insertBefore(scriptElement, vizElement);                &lt;/script&gt;

&lt;p&gt;The initial dashboard created - now titled ‘Top 10 Events and Trends by Selected Date Level’ - presented high level summaries on which events generated the most calls for service, monthly trends for top 10 events, and a breakdown of event types for locations with the most events. It has since been updated with filters for event type, beat, and month and year, as well as the option to select granularity for the time series (e.g. year, month, week, day) and adjust date range displayed.&lt;/p&gt;

&lt;p&gt;At the time the dashboard was created, the initial dashboard showed a couple disparities such as:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Traffic stops occurring 3 to 4 times then the remaining top 10 events&lt;/li&gt;
  &lt;li&gt;Locations such as Washington Blvd (AKA Route 1) generating the most calls for service - heavily traffic stops - followed by obvious locations such as the county courthouse and police department, primarily comprised of administrative and special assignments.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Although obvious from the visualization, these trends are ultimately not very informative or interesting and with so many event types and locations it is difficult to tease out additional trends.&lt;/p&gt;

&lt;h2 id=&quot;issues-with-data-quality-and-coding&quot;&gt;Issues with Data Quality and Coding&lt;/h2&gt;
&lt;p&gt;Several issues identified originally when creating this visualization were in data quality or coding, including:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Inconsistent coding of the ‘Date Reported’ column (e.g., 01/01/2014 versus 01-Jan-2015), causing the majority of data to be excluded when charting time series data. (UPDATE: This issue appears to have been addressed in the most recent update to the dataset on the open data portal)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Broad versus specialized categories for ‘Computer Aided Dispatch Event Type’. TRAFFIC STOP event type as a potential catch-all category, as we see more specific events such as DWI, LICENSE PLATE READER, M/V ACCIDENT, MOTOR VEHICLE VIOLATION, PROP DAMAGE, TRAFFIC HAZARD, TRAFFIC PURSUIT, VEH CRASH that could be interpreted as special sub-category of TRAFFIC STOP. For example, when viewing monthly trends there are drops in TRAFFIC STOP events where there are spikes in TRAFFIC HAZARD or MOTOR VEHICLE VIOLATION events. This leads one to wonder whether there was actually a decrease or just that events were inconsistenly coded in each time period. With so many categories, it could be useful to develop a heirarchy so that major event categories could be summarized at a high level, allowing the user to further drill down into more specific sub-categories. Documentation on the exact meaning and methodology for each event type would also be useful for developers and data entry alike.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Location field is inconsistent in it’s use of primary roads, intersections, or XY coordinates. WASHINGTON BLVD (also known as Route 1) is particularly problematic as it stretches across the entire county and state and therefore dominates other entries when visualized. The dataset is filled with numerous entries including WASHINGTON BLVD, RT 1, or some intersection of WASHINGTON BLVD/RT 1. For greater flexibility in reporting and visualization, the dataset would benefit from a dedicated XY coordinates column, alleviating in part the issue of consistency for the Location field.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Also recorded for each event is the assigned Beat and Statistical Reporting Area, which provide information on the general areas or neighborhoods in which each event occurred. At this time there is no provided documentation or spatial data files (i.e., shape files, KML, geojson) indicating the boundaries for these geographic regions. To provide more high quality reporting capability, I set out to create a custom geospatial data file.&lt;/p&gt;

&lt;h2 id=&quot;creating-custom-spatial-data-file&quot;&gt;Creating Custom Spatial Data File&lt;/h2&gt;

&lt;p&gt;Although the County was unable to provide a spatial data file (e.g. shp, kml, geojson), they did provide the below image with Beat assignments in Howard County, with areas and boundaries clearly labelled and colored. Since I am working in a Mac environment and Arc GIS is primarily designed for Windows, I made the decision to work with Google Earth, free and open source, to create a KML file.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/HCPD-Tableau/Beats.png&quot; alt=&quot;Howard County Police Dept Beats&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The process begins by importing the image into Google Earth as an image overlay and dragging the corners of the image to stretch and skew it until it matches the county boundaries as closely as possible.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/HCPD-Tableau/google-earth-map-overlay.png&quot; alt=&quot;Screenshot, Google Earth Image Overlay of Howard County Police Dept Beats&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We then draw polygons in Google Earth for each of the Beat assignments, creating points along the boundaries to draw complex polygons. Name and description are provided for each of the polygons, which will be included in the KML export. The below screenshot shows ‘New Polygon’ dialog which remains open as the polygon is drawn.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/HCPD-Tableau/google-earth-draw-polygon.png&quot; alt=&quot;Screenshot, Google Earth Draw Polygon feature&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Once all polygons are created, I exported ‘My Places’ from Google Earth as a KML file, which can then be imported into Tableau as a spatial data file. The initial import was problematic as Tableau noted that KML should provide all placemarks/polygons on a single layer. Opening the KML file within a text editor, I simply move the series of Placemark XML tags outside of their Folder parent tag and reattempted import, successfully.&lt;/p&gt;

&lt;h2 id=&quot;mapping-events-by-beat&quot;&gt;Mapping Events by Beat&lt;/h2&gt;

&lt;p&gt;Tableau allows imports of spatial data files as a dataset and once imported can be joined or blended with the primary data source. The imported dataset includes fields for name and description as specified in Google Earth, as well as geometries for the polygons that were drawn. In this case, we link ‘Beat’ from the primary data source with ‘Name’ from the imported KML file. A custom fill map (AKA choropleth map) may then be created by double clicking the Geometry field from the spatial data file to import the appropriate polygons. From there the visualization is created as you would with a typical fill map in Tableau. Coloring is done based on record counts for a given month and year. Filters provided allow the user to examine number of events for a selected month and year, beat, or computer aided dispatch event type on both the fill map as well as an accompanying cross tab with all the individual records.&lt;/p&gt;

&lt;h2 id=&quot;future-research-and-improvements&quot;&gt;Future Research and Improvements&lt;/h2&gt;

&lt;p&gt;This mockup was particularly instructive for me in creating custom spatial data files from scratch. I found Google Earth relatively easy to work with although the drawing of the custom polygons was quite cumbersome and Google Earth seemed to lack some basic tools such as the ability to erase points from a polygon once they were drawn, meaning if mistakes were made you had to delete the shape and begin again. I’ll be interested in the future to test additional tools that generate, for example, geoJSON files which can be more easily utilized in JavaScript mapping libraries such as Leaflet.&lt;/p&gt;

&lt;p&gt;Although it was not the primary focus of this exercise, more effort could be spent to improve the high level summary of the second storyboard showing top events, trends over time, and events by location. Some ideas include:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Filters to show top events types and monthly trends for a particular beat (UPDATED: 12/21/2017)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Dropdown to select level of granularity for time series. The final design settled on monthly numbers, but additional charts by week or day could be developed and swapped in and out through a parameter dropdown (UPDATED 12/21/2017)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;With better understanding of the event types, create a heirarchy that allows the visualization to drill from broad, overarching categories to more specialized event types. Visualizations such as tree maps or bar charts could then be used to select and drill down into a particular category, which would reduce the amount of event types under consideration and better facilitate identification of trends.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>Anthony Lipphardt</name></author><summary type="html">This visualization, created with Tableau Public, incorporates data on Howard County Police Department Calls for Service (2014-2016) and started as part of a final exam for my Business Analytics and Strategic Decision Making class, which asked to create a data visualization in Tableau with a dataset of my choosing.</summary></entry><entry><title type="html">Twitter Bot Project</title><link href="http://localhost:4000/2017/11/30/twitterbot.html" rel="alternate" type="text/html" title="Twitter Bot Project" /><published>2017-11-30T16:27:59-05:00</published><updated>2017-11-30T16:27:59-05:00</updated><id>http://localhost:4000/2017/11/30/twitterbot</id><content type="html" xml:base="http://localhost:4000/2017/11/30/twitterbot.html">&lt;p&gt;This project stems from a Twitter post from my good friend Andrew Naber, who jokingly suggested the creation of a bot that sends the ‘Old Man Yells at Cloud’ every time a news article about millennials appears on the web. Having just covered web scraping and APIs in my &lt;em&gt;Programming for Data Science&lt;/em&gt; class, I happily obliged. Thanks to &lt;a href=&quot;https://www.linkedin.com/in/noah-diekemper-46152492/&quot;&gt;Noah Diekemper&lt;/a&gt; for introducing and demoing the News API.&lt;/p&gt;

&lt;blockquote class=&quot;twitter-tweet&quot; data-lang=&quot;en&quot;&gt;&lt;p lang=&quot;en&quot; dir=&quot;ltr&quot;&gt;I want to write a bot script, where it finds if the keyword of an article is, &amp;quot;Millennial&amp;quot;, and it sends the &amp;quot;Old Man Yells at Cloud&amp;quot; pic. &lt;a href=&quot;https://t.co/8arrCRftTu&quot;&gt;pic.twitter.com/8arrCRftTu&lt;/a&gt;&lt;/p&gt;&amp;mdash; Andrew M Naber (@AndrewMNaber) &lt;a href=&quot;https://twitter.com/AndrewMNaber/status/927972980210139138?ref_src=twsrc%5Etfw&quot;&gt;November 7, 2017&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async=&quot;&quot; src=&quot;https://platform.twitter.com/widgets.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;

&lt;blockquote class=&quot;twitter-tweet&quot; data-conversation=&quot;none&quot; data-lang=&quot;en&quot;&gt;&lt;p lang=&quot;en&quot; dir=&quot;ltr&quot;&gt;Your wish is my command. I will use &lt;a href=&quot;https://t.co/mcZDHOZiQW&quot;&gt;https://t.co/mcZDHOZiQW&lt;/a&gt; &amp;amp; twitter for my programming for Data Science class to create this.&lt;/p&gt;&amp;mdash; Anthony Lipphardt (@a_lipphardt) &lt;a href=&quot;https://twitter.com/a_lipphardt/status/929080275715461120?ref_src=twsrc%5Etfw&quot;&gt;November 10, 2017&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async=&quot;&quot; src=&quot;https://platform.twitter.com/widgets.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;

&lt;p&gt;This twitter bot, written in python, is configured to search for a given a term - ‘millennial’ in this case - and scrapes the News API and Google RSS feed for articles containing that term. Long URLs are passed to the Bitly API for shortening and a status update with media (e.g., ‘Old Man Yells at Cloud’ picture) is then posted to the given Twitter profile using the Tweepy python library.&lt;/p&gt;

&lt;p&gt;Code for this project is included in my &lt;a href=&quot;https://github.com/alipphardt/twitterbot&quot;&gt;twitterbot repo&lt;/a&gt; on GitHub. Access keys/tokens have been removed from the script. In order to get the script to work, developer tokens must be requested from &lt;a href=&quot;https://developer.twitter.com/&quot;&gt;Twitter&lt;/a&gt; and &lt;a href=&quot;https://dev.bitly.com/&quot;&gt;Bitly&lt;/a&gt; and copied into the script.&lt;/p&gt;

&lt;p&gt;An example of the Twitter-bot using this script can be found at &lt;a href=&quot;https://twitter.com/millennialYell&quot;&gt;https://twitter.com/millennialYell&lt;/a&gt;.&lt;/p&gt;

&lt;blockquote class=&quot;twitter-tweet&quot; data-lang=&quot;en&quot;&gt;&lt;p lang=&quot;en&quot; dir=&quot;ltr&quot;&gt;&lt;a href=&quot;https://t.co/6OL1w0w9gu&quot;&gt;https://t.co/6OL1w0w9gu&lt;/a&gt; Millennials Plan to Redefine the C-Suite &lt;a href=&quot;https://t.co/fEY6byHQUU&quot;&gt;pic.twitter.com/fEY6byHQUU&lt;/a&gt;&lt;/p&gt;&amp;mdash; Grandpa Simpson (@millennialYell) &lt;a href=&quot;https://twitter.com/millennialYell/status/936279603089412096?ref_src=twsrc%5Etfw&quot;&gt;November 30, 2017&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async=&quot;&quot; src=&quot;https://platform.twitter.com/widgets.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;

&lt;p&gt;In order to run the script on a schedule, I use the hosting service &lt;a href=&quot;http://www.pythonanywhere.com&quot;&gt;pythonanywhere.com&lt;/a&gt; which supports running of python scripts. Free accounts are currently available and include one daily scheduled task.&lt;/p&gt;

&lt;p&gt;The first version of this bot is limited in that it only posts the shortened bitly link and the article title. Future improvements might include:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Code Refactoring&lt;/strong&gt; - Improve readability and usability of code, specifically by creating a &lt;strong&gt;Bot&lt;/strong&gt; class which can be configured when instantiated with term and persona settings, as well as encapsulating key functions such as scraping and tweeting.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Inclusion of twitter handle for author organization&lt;/strong&gt; - Lookup tables could be stored and queried via SQLite, mapping domain names (e.g. cnn.com) to twitter handles (e.g. @CNN). If a mapping does not exist enter a record for the domain which would then be filled at a later time. At the moment, the Twitter API  doesn’t seem to have the capability to search and return user IDs, so the process of adding the twitter handles would be manual in nature.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Automated quotes for bot persona depending on article title&lt;/strong&gt; - This might be accomplished through some basic machine learning algorithms such as decision trees. Potentially a good project for next semesters &lt;em&gt;Machine Learning&lt;/em&gt; class.&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Anthony Lipphardt</name></author><summary type="html">This project stems from a Twitter post from my good friend Andrew Naber, who jokingly suggested the creation of a bot that sends the ‘Old Man Yells at Cloud’ every time a news article about millennials appears on the web. Having just covered web scraping and APIs in my Programming for Data Science class, I happily obliged. Thanks to Noah Diekemper for introducing and demoing the News API.</summary></entry><entry><title type="html">CDC WONDER API - Detailed Mortality Database Example</title><link href="http://localhost:4000/2017/10/14/cdc-wonder-api-example.html" rel="alternate" type="text/html" title="CDC WONDER API - Detailed Mortality Database Example" /><published>2017-10-14T00:00:00-04:00</published><updated>2017-10-14T00:00:00-04:00</updated><id>http://localhost:4000/2017/10/14/cdc-wonder-api-example</id><content type="html" xml:base="http://localhost:4000/2017/10/14/cdc-wonder-api-example.html">&lt;iframe width=&quot;750&quot; height=&quot;450&quot; src=&quot;https://www.youtube.com/embed/ttchrtCZ46Y?rel=0&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://wonder.cdc.gov/&quot;&gt;CDC WONDER&lt;/a&gt; is a query tool from the Centers for Disease Control (CDC) that provides access to a collection of online databases for the analysis of public health data.&lt;/p&gt;

&lt;p&gt;The following are a sampling of databases available through WONDER that provide vital statistics data through CDC’s National Center for Health Statistics&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://wonder.cdc.gov/natality.html&quot;&gt;Births&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://wonder.cdc.gov/ucd-icd10.html&quot;&gt;Detailed Mortality&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://wonder.cdc.gov/mortSQL.html&quot;&gt;Compressed Mortality&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://wonder.cdc.gov/mcd.html&quot;&gt;Multiple cause of death&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://wonder.cdc.gov/lbd.html&quot;&gt;Infant Deaths&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For this example, we will focus on the &lt;a href=&quot;https://wonder.cdc.gov/ucd-icd10.html&quot;&gt;Detailed Mortality&lt;/a&gt; database, which provides number of deaths and death rates (crude or age-adjusted) for underlying cause of death at the national, state and county levels. Using the query tool, the user can select grouping and filtering variables that are use to generate a dataset. Results are provided as a data table which can then be exported to a tab delimited file or visualized.&lt;/p&gt;

&lt;p&gt;WONDER provides an API that allows the same queries (with some limitations) to be issued through a POST request to WONDER’s web server. Requests and responses are issued in XML format and are detailed in the &lt;a href=&quot;https://wonder.cdc.gov/wonder/help/WONDER-API.html&quot;&gt;API Documentation&lt;/a&gt; page.&lt;/p&gt;

&lt;p&gt;Each XML request consists of a series of parameter tags with name and value children in the following format:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;lt;request-parameters&amp;gt;
    &amp;lt;parameter&amp;gt;
        &amp;lt;name&amp;gt;&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;&amp;lt;/value&amp;gt;
    &amp;lt;parameter&amp;gt;
    ...
&amp;lt;/request-parameters&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;A reference of available parameter names and values can be found on my &lt;a href=&quot;https://github.com/alipphardt/cdc-wonder-api&quot;&gt;cdc-wonder-api repo README&lt;/a&gt;. Additional references for the other databases may be added in the future.&lt;/p&gt;

&lt;h2 id=&quot;creating-a-request&quot;&gt;Creating a Request&lt;/h2&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# by-variables&quot; or those parameters selected in the &quot;Group Results By&quot; and the &quot;And By&quot; drop-down lists &lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# in the &quot;Request Form.&quot; These &quot;by-variables&quot; are the cross-tabulations, stratifications or indexes &lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# to the query results. Expect the results data table to show a row for each category in the by-variables, &lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# and a column for each measure. For example, if you wish to compare data by sex, then &quot;group results by&quot; gender, &lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# to get a row for females and a row for males in the output.&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# For this example, will group by year and race&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;b_parameters&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;B_1&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;D76.V1-level1&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
    &lt;span class=&quot;s&quot;&gt;&quot;B_2&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;D76.V8&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
    &lt;span class=&quot;s&quot;&gt;&quot;B_3&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;*None*&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
    &lt;span class=&quot;s&quot;&gt;&quot;B_4&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;*None*&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
    &lt;span class=&quot;s&quot;&gt;&quot;B_5&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;*None*&quot;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# M paremeters are measures to return, the default measures plus any optional measures.&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# For this example, include deaths, population, and crude rate&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;m_parameters&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;M_1&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;D76.M1&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;   &lt;span class=&quot;c&quot;&gt;# Deaths, must be included&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;M_2&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;D76.M2&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;   &lt;span class=&quot;c&quot;&gt;# Population, must be included&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;M_3&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;D76.M3&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;   &lt;span class=&quot;c&quot;&gt;# Crude rate, must be included&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;#&quot;M_31&quot;: &quot;D76.M31&quot;,        # Standard error (crude rate)&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;#&quot;M_32&quot;: &quot;D76.M32&quot;         # 95% confidence interval (crude rate)&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;M_41&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;D76.M41&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Standard error (age-adjusted rate)&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;M_42&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;D76.M42&quot;&lt;/span&gt;  &lt;span class=&quot;c&quot;&gt;# 95% confidence interval (age-adjusted rate)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# Values highlighted in a &quot;Finder&quot; control for hierarchical lists, &lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# such as the &quot;Regions/Divisions/States/Counties hierarchical&quot; list.&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# For this example, include all years, months, census regions, hhs regions, states. Only include ICD-10 K00-K92&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# for disease of the digestive system&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;f_parameters&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;F_D76.V1&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;*All*&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# year/month&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;F_D76.V10&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;*All*&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Census Regions - dont change&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;F_D76.V2&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;K00-K92&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# ICD-10 Codes&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;F_D76.V27&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;*All*&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# HHS Regions - dont change&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;F_D76.V9&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;*All*&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# State County - dont change&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# Contents of the &quot;Currently selected&quot; information areas next to &quot;Finder&quot; controls in the &quot;Request Form.&quot;&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# For this example, include all dates, census regions, hhs regions, and states.&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Only include ICD-10 code K00-K92 for disease of the digestive system&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;i_parameters&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;I_D76.V1&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;*All* (All Dates)&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;c&quot;&gt;# year/month&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;I_D76.V10&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;*All* (The United States)&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Census Regions - dont change&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;I_D76.V2&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;K00-K92 (Diseases of the digestive system)&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# ICD-10 Codes&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;I_D76.V27&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;*All* (The United States)&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# HHS Regions - dont change&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;I_D76.V9&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;*All* (The United States)&quot;&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# State County - dont change&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# Variable values to limit in the &quot;where&quot; clause of the query, found in multiple select &lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# list boxes and advanced finder text entry boxes in the &quot;Request Form.&quot;&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# For this example, we want to include ten-year age groups for ages 15-44.&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# For all other categories, include all values&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;v_parameters&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;V_D76.V1&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;         &lt;span class=&quot;c&quot;&gt;# Year/Month&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;V_D76.V10&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;        &lt;span class=&quot;c&quot;&gt;# Census Regions&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;V_D76.V11&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;*All*&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;   &lt;span class=&quot;c&quot;&gt;# 2006 Urbanization&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;V_D76.V12&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;*All*&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;   &lt;span class=&quot;c&quot;&gt;# ICD-10 130 Cause List (Infants)&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;V_D76.V17&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;*All*&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;   &lt;span class=&quot;c&quot;&gt;# Hispanic Origin&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;V_D76.V19&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;*All*&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;   &lt;span class=&quot;c&quot;&gt;# 2013 Urbanization&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;V_D76.V2&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;         &lt;span class=&quot;c&quot;&gt;# ICD-10 Codes&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;V_D76.V20&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;*All*&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;   &lt;span class=&quot;c&quot;&gt;# Autopsy&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;V_D76.V21&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;*All*&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;   &lt;span class=&quot;c&quot;&gt;# Place of Death&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;V_D76.V22&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;*All*&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;   &lt;span class=&quot;c&quot;&gt;# Injury Intent&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;V_D76.V23&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;*All*&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;   &lt;span class=&quot;c&quot;&gt;# Injury Mechanism and All Other Leading Causes&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;V_D76.V24&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;*All*&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;   &lt;span class=&quot;c&quot;&gt;# Weekday&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;V_D76.V25&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;*All*&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;   &lt;span class=&quot;c&quot;&gt;# Drug/Alcohol Induced Causes&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;V_D76.V27&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;        &lt;span class=&quot;c&quot;&gt;# HHS Regions&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;V_D76.V4&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;*All*&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;    &lt;span class=&quot;c&quot;&gt;# ICD-10 113 Cause List&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;V_D76.V5&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;15-24&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;25-34&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;35-44&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Ten-Year Age Groups&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;V_D76.V51&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;*All*&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;   &lt;span class=&quot;c&quot;&gt;# Five-Year Age Groups&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;V_D76.V52&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;*All*&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;   &lt;span class=&quot;c&quot;&gt;# Single-Year Ages&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;V_D76.V6&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;00&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;       &lt;span class=&quot;c&quot;&gt;# Infant Age Groups&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;V_D76.V7&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;*All*&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;    &lt;span class=&quot;c&quot;&gt;# Gender&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;V_D76.V8&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;*All*&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;    &lt;span class=&quot;c&quot;&gt;# Race&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;V_D76.V9&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;&quot;&lt;/span&gt;          &lt;span class=&quot;c&quot;&gt;# State/County&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# Other parameters, such as radio buttons, checkboxes, and lists that are not data categories&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# For this example, include age-adjusted rates, use ten-year age groups (D76.V5), use state location by default, &lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# show rates per 100,000, use 2013 urbanization and use ICD-10 Codes (D76.V2) for cause of death category&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;o_parameters&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;O_V10_fmode&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;freg&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;    &lt;span class=&quot;c&quot;&gt;# Use regular finder and ignore v parameter value&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;O_V1_fmode&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;freg&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;     &lt;span class=&quot;c&quot;&gt;# Use regular finder and ignore v parameter value&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;O_V27_fmode&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;freg&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;    &lt;span class=&quot;c&quot;&gt;# Use regular finder and ignore v parameter value&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;O_V2_fmode&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;freg&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;     &lt;span class=&quot;c&quot;&gt;# Use regular finder and ignore v parameter value&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;O_V9_fmode&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;freg&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;     &lt;span class=&quot;c&quot;&gt;# Use regular finder and ignore v parameter value&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;O_aar&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;aar_std&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;       &lt;span class=&quot;c&quot;&gt;# age-adjusted rates&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;O_aar_pop&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;0000&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;      &lt;span class=&quot;c&quot;&gt;# population selection for age-adjusted rates&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;O_age&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;D76.V5&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;        &lt;span class=&quot;c&quot;&gt;# select age-group (e.g. ten-year, five-year, single-year, infant groups)&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;O_javascript&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;on&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;     &lt;span class=&quot;c&quot;&gt;# Set to on by default&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;O_location&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;D76.V9&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;   &lt;span class=&quot;c&quot;&gt;# select location variable to use (e.g. state/county, census, hhs regions)&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;O_precision&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;1&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;       &lt;span class=&quot;c&quot;&gt;# decimal places&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;O_rate_per&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;100000&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;   &lt;span class=&quot;c&quot;&gt;# rates calculated per X persons&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;O_show_totals&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;false&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;c&quot;&gt;# Show totals for &lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;O_timeout&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;300&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;O_title&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Digestive Disease Deaths, by Age Group&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;    &lt;span class=&quot;c&quot;&gt;# title for data run&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;O_ucd&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;D76.V2&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;        &lt;span class=&quot;c&quot;&gt;# select underlying cause of death category&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;O_urban&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;D76.V19&quot;&lt;/span&gt;      &lt;span class=&quot;c&quot;&gt;# select urbanization category&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# Values for non-standard age adjusted rates (see mortality online databases).&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# For this example, these parameters are ignored as standard age adjusted rates are used&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;vm_parameters&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;VM_D76.M6_D76.V10&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;        &lt;span class=&quot;c&quot;&gt;# Location&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;VM_D76.M6_D76.V17&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;*All*&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;   &lt;span class=&quot;c&quot;&gt;# Hispanic-Origin&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;VM_D76.M6_D76.V1_S&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;*All*&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;c&quot;&gt;# Year&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;VM_D76.M6_D76.V7&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;*All*&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;    &lt;span class=&quot;c&quot;&gt;# Gender&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;VM_D76.M6_D76.V8&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;*All*&quot;&lt;/span&gt;     &lt;span class=&quot;c&quot;&gt;# Race&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# Miscellaneous hidden inputs/parameters usually passed by web form. These do not change.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;misc_parameters&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;action-Send&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Send&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;finder-stage-D76.V1&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;codeset&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;finder-stage-D76.V1&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;codeset&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;finder-stage-D76.V2&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;codeset&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;finder-stage-D76.V27&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;codeset&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;finder-stage-D76.V9&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;codeset&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;stage&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;request&quot;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;createParameterList&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parameterList&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;Helper function to create a parameter list from a dictionary object&quot;&quot;&quot;&lt;/span&gt;
    
    &lt;span class=&quot;n&quot;&gt;parameterString&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;&quot;&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;key&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;parameterList&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;parameterString&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;&amp;lt;parameter&amp;gt;&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;parameterString&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;&amp;lt;name&amp;gt;&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;key&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;&amp;lt;/name&amp;gt;&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;
        
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;isinstance&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parameterList&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;value&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;parameterList&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;parameterString&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;&amp;lt;value&amp;gt;&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;value&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;&amp;lt;/value&amp;gt;&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;parameterString&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;&amp;lt;value&amp;gt;&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;parameterList&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;&amp;lt;/value&amp;gt;&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;
        
        &lt;span class=&quot;n&quot;&gt;parameterString&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;&amp;lt;/parameter&amp;gt;&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;
        
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;parameterString&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;xml_request&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;&amp;lt;request-parameters&amp;gt;&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;xml_request&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;createParameterList&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b_parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;xml_request&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;createParameterList&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m_parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;xml_request&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;createParameterList&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f_parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;xml_request&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;createParameterList&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i_parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;xml_request&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;createParameterList&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;o_parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;xml_request&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;createParameterList&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vm_parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;xml_request&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;createParameterList&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v_parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;xml_request&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;createParameterList&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;misc_parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;xml_request&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;&amp;lt;/request-parameters&amp;gt;&quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;sending-the-request&quot;&gt;Sending the Request&lt;/h2&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;requests&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;url&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;https://wonder.cdc.gov/controller/datarequest/D76&quot;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;response&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;requests&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;post&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;url&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;request_xml&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xml_request&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;accept_datause_restrictions&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;true&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;response&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;status_code&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;200&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;response&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;text&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;something went wrong&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;processing-the-request&quot;&gt;Processing the Request&lt;/h2&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# BeautifulSoup library facilitates parsing of XML response&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;bs4&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bs&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# This library faciliates 2-dimensional array operations and visualization&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pandas&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;xml2df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xml_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot; This function grabs the root of the XML document and iterates over
        the 'r' (row) and 'c' (column) tags of the data-table
        Rows with a 'v' attribute contain a numerical value
        Rows with a 'l attribute contain a text label and may contain an
        additional 'r' (rowspan) tag which identifies how many rows the value
        should be added. If present, that label will be added to the following
        rows of the data table.
    
        Function returns a two-dimensional array or data frame that may be 
        used by the pandas library.&quot;&quot;&quot;&lt;/span&gt;
    
    &lt;span class=&quot;n&quot;&gt;root&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;BeautifulSoup&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xml_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;lxml&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;all_records&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;row_number&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;rows&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;root&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;find_all&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;r&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;row&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rows&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;row_number&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;all_records&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;all_records&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([])&lt;/span&gt;
              
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cell&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;row&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;find_all&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;c&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'v'&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cell&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;attrs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
                &lt;span class=&quot;k&quot;&gt;try&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;all_records&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;row_number&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cell&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;attrs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;v&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;replace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;','&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;''&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
                &lt;span class=&quot;k&quot;&gt;except&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;ValueError&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;all_records&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;row_number&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cell&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;attrs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;v&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
                &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'r'&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cell&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;attrs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;all_records&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;row_number&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cell&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;attrs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;l&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
                &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
                
                    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;row_index&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cell&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;attrs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;r&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])):&lt;/span&gt;
                        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;row_number&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;row_index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;all_records&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
                            &lt;span class=&quot;n&quot;&gt;all_records&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([])&lt;/span&gt;
                            &lt;span class=&quot;n&quot;&gt;all_records&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;row_number&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;row_index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cell&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;attrs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;l&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
                        &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
                            &lt;span class=&quot;n&quot;&gt;all_records&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;row_number&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;row_index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cell&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;attrs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;l&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
                                           
        &lt;span class=&quot;n&quot;&gt;row_number&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;all_records&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;data_frame&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xml2df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DataFrame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data_frame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;columns&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Year&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Race&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Deaths&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Population&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Crude Rate&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Age-adjusted Rate&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Age-adjusted Rate Standard Error&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;head&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div&gt;
&lt;style&gt;

    .dataframe {
        margin-top: 20px;
        margin-bottom: 20px;
    }

    .dataframe thead tr:only-child th {
        text-align: right;
    }

    .dataframe thead th {
        text-align: left;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
&lt;/style&gt;
&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt;
  &lt;thead&gt;
    &lt;tr style=&quot;text-align: right;&quot;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Year&lt;/th&gt;
      &lt;th&gt;Race&lt;/th&gt;
      &lt;th&gt;Deaths&lt;/th&gt;
      &lt;th&gt;Population&lt;/th&gt;
      &lt;th&gt;Crude Rate&lt;/th&gt;
      &lt;th&gt;Age-adjusted Rate&lt;/th&gt;
      &lt;th&gt;Age-adjusted Rate Standard Error&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;1999&lt;/td&gt;
      &lt;td&gt;American Indian or Alaska Native&lt;/td&gt;
      &lt;td&gt;210.0&lt;/td&gt;
      &lt;td&gt;1375207.0&lt;/td&gt;
      &lt;td&gt;15.3&lt;/td&gt;
      &lt;td&gt;17.2&lt;/td&gt;
      &lt;td&gt;1.2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;1999&lt;/td&gt;
      &lt;td&gt;Asian or Pacific Islander&lt;/td&gt;
      &lt;td&gt;73.0&lt;/td&gt;
      &lt;td&gt;5813970.0&lt;/td&gt;
      &lt;td&gt;1.3&lt;/td&gt;
      &lt;td&gt;1.3&lt;/td&gt;
      &lt;td&gt;0.2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;1999&lt;/td&gt;
      &lt;td&gt;Black or African American&lt;/td&gt;
      &lt;td&gt;1176.0&lt;/td&gt;
      &lt;td&gt;17026405.0&lt;/td&gt;
      &lt;td&gt;6.9&lt;/td&gt;
      &lt;td&gt;7.4&lt;/td&gt;
      &lt;td&gt;0.2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;1999&lt;/td&gt;
      &lt;td&gt;White&lt;/td&gt;
      &lt;td&gt;5067.0&lt;/td&gt;
      &lt;td&gt;99715532.0&lt;/td&gt;
      &lt;td&gt;5.1&lt;/td&gt;
      &lt;td&gt;5.1&lt;/td&gt;
      &lt;td&gt;0.1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;2000&lt;/td&gt;
      &lt;td&gt;American Indian or Alaska Native&lt;/td&gt;
      &lt;td&gt;213.0&lt;/td&gt;
      &lt;td&gt;1438695.0&lt;/td&gt;
      &lt;td&gt;14.8&lt;/td&gt;
      &lt;td&gt;16.4&lt;/td&gt;
      &lt;td&gt;1.1&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;h2 id=&quot;basic-visualizations&quot;&gt;Basic Visualizations&lt;/h2&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# Load matplotlib for plotting and instruct jupyter to display figures inline&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pyplot&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;matplotlib&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inline&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Group total number of deaths by year&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;groupby&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;by&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Year'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Deaths'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Deaths from Digestive Disease: United States'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gcf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_size_inches&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/images/CDC%252BWONDER%252BAPI%252BExample_files/CDC%252BWONDER%252BAPI%252BExample_19_0.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# Create mult-line chart for deaths by race &lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Store figure and axis for shared plot&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;subplots&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Store labels for all race groups&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# For each group in the groupby object, grab the 'Race' label and create a line plot for it&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grp&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;groupby&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Race'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grp&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'line'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Year'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Deaths'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Set the labels for each line using the group labels&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;lines&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_legend_handles_labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;legend&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lines&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'best'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Configure chart size and title&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gcf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_size_inches&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Deaths from Digestive Disease, by Race: United States&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/images/CDC%252BWONDER%252BAPI%252BExample_files/CDC%252BWONDER%252BAPI%252BExample_20_0.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# New query, this time we wish to group by year, month, and cause of death&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;b_parameters&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;B_1&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;D76.V1-level1&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Year&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;B_2&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;D76.V1-level2&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Month &lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;B_3&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;D76.V2-level3 &quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;c&quot;&gt;# Cause of death &lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;B_4&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;*None*&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
    &lt;span class=&quot;s&quot;&gt;&quot;B_5&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;*None*&quot;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;f_parameters&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;F_D76.V1&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;*All*&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# year/month&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;F_D76.V10&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;*All*&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Census Regions - dont change&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;F_D76.V2&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;X40&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;X41&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;X42&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;X43&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;X44&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;X60&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;X61&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;X62&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;X63&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;X64&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;X85&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Y10&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Y11&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Y12&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Y13&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Y14&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; 
        &lt;span class=&quot;c&quot;&gt;# ICD-10 Codes - Drug overdose deaths are identified using ICD–10 underlying cause-of-death codes: &lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;# X40–X44, X60–X64, X85, and Y10–Y14.&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;F_D76.V27&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;*All*&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# HHS Regions - dont change&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;F_D76.V9&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;*All*&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# State County - dont change&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;v_parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;V_D76.V5&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;*All*&quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;xml_request&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;&amp;lt;request-parameters&amp;gt;&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;xml_request&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;createParameterList&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b_parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;xml_request&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;createParameterList&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m_parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;xml_request&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;createParameterList&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f_parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;xml_request&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;createParameterList&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i_parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;xml_request&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;createParameterList&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;o_parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;xml_request&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;createParameterList&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vm_parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;xml_request&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;createParameterList&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v_parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;xml_request&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;createParameterList&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;misc_parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;xml_request&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;&amp;lt;/request-parameters&amp;gt;&quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;url&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;https://wonder.cdc.gov/controller/datarequest/D76&quot;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;response&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;requests&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;post&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;url&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;request_xml&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xml_request&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;accept_datause_restrictions&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;true&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;response&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;status_code&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;200&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;response&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;text&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;something went wrong&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;data_frame&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xml2df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DataFrame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data_frame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;columns&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Year&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Month&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Cause of Death&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Deaths&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Population&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Crude Rate&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Age-adjusted Rate&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Age-adjusted Rate Standard Error&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;head&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div&gt;
&lt;style&gt;
    .dataframe thead tr:only-child th {
        text-align: right;
    }

    .dataframe thead th {
        text-align: left;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
&lt;/style&gt;
&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt;
  &lt;thead&gt;
    &lt;tr style=&quot;text-align: right;&quot;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Year&lt;/th&gt;
      &lt;th&gt;Month&lt;/th&gt;
      &lt;th&gt;Cause of Death&lt;/th&gt;
      &lt;th&gt;Deaths&lt;/th&gt;
      &lt;th&gt;Population&lt;/th&gt;
      &lt;th&gt;Crude Rate&lt;/th&gt;
      &lt;th&gt;Age-adjusted Rate&lt;/th&gt;
      &lt;th&gt;Age-adjusted Rate Standard Error&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;1999&lt;/td&gt;
      &lt;td&gt;Jan., 1999&lt;/td&gt;
      &lt;td&gt;Accidental poisoning by and exposure to nonopi...&lt;/td&gt;
      &lt;td&gt;19.0&lt;/td&gt;
      &lt;td&gt;Not Applicable&lt;/td&gt;
      &lt;td&gt;Not Applicable&lt;/td&gt;
      &lt;td&gt;Not Applicable&lt;/td&gt;
      &lt;td&gt;Not Applicable&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;1999&lt;/td&gt;
      &lt;td&gt;Jan., 1999&lt;/td&gt;
      &lt;td&gt;Accidental poisoning by and exposure to antiep...&lt;/td&gt;
      &lt;td&gt;47.0&lt;/td&gt;
      &lt;td&gt;Not Applicable&lt;/td&gt;
      &lt;td&gt;Not Applicable&lt;/td&gt;
      &lt;td&gt;Not Applicable&lt;/td&gt;
      &lt;td&gt;Not Applicable&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;1999&lt;/td&gt;
      &lt;td&gt;Jan., 1999&lt;/td&gt;
      &lt;td&gt;Accidental poisoning by and exposure to narcot...&lt;/td&gt;
      &lt;td&gt;460.0&lt;/td&gt;
      &lt;td&gt;Not Applicable&lt;/td&gt;
      &lt;td&gt;Not Applicable&lt;/td&gt;
      &lt;td&gt;Not Applicable&lt;/td&gt;
      &lt;td&gt;Not Applicable&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;1999&lt;/td&gt;
      &lt;td&gt;Jan., 1999&lt;/td&gt;
      &lt;td&gt;Accidental poisoning by and exposure to other ...&lt;/td&gt;
      &lt;td&gt;4.0&lt;/td&gt;
      &lt;td&gt;Not Applicable&lt;/td&gt;
      &lt;td&gt;Not Applicable&lt;/td&gt;
      &lt;td&gt;Not Applicable&lt;/td&gt;
      &lt;td&gt;Not Applicable&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;1999&lt;/td&gt;
      &lt;td&gt;Jan., 1999&lt;/td&gt;
      &lt;td&gt;Accidental poisoning by and exposure to other ...&lt;/td&gt;
      &lt;td&gt;370.0&lt;/td&gt;
      &lt;td&gt;Not Applicable&lt;/td&gt;
      &lt;td&gt;Not Applicable&lt;/td&gt;
      &lt;td&gt;Not Applicable&lt;/td&gt;
      &lt;td&gt;Not Applicable&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;groupby&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;by&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Year'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;head&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div&gt;
&lt;style&gt;
    .dataframe thead tr:only-child th {
        text-align: right;
    }

    .dataframe thead th {
        text-align: left;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
&lt;/style&gt;
&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt;
  &lt;thead&gt;
    &lt;tr style=&quot;text-align: right;&quot;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Deaths&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Year&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;1999&lt;/th&gt;
      &lt;td&gt;16849.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2000&lt;/th&gt;
      &lt;td&gt;17415.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2001&lt;/th&gt;
      &lt;td&gt;19394.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2002&lt;/th&gt;
      &lt;td&gt;23518.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2003&lt;/th&gt;
      &lt;td&gt;25785.0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;df_groups&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;groupby&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;by&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Year'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Cause of Death'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;as_index&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;df_pivot&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df_groups&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pivot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;index&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Year'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;columns&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Cause of Death'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;values&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Deaths'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df_pivot&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;area&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Drug overdose deaths: United States'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;legend_labels&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;X40&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;X41&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;X42&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;X43&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;X44&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;X60&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;X61&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;X62&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;X63&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;X64&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;X85&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Y10&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Y11&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Y12&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Y13&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Y14&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;legend&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;legend_labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_ylabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Deaths per 100,000&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gcf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_size_inches&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/images/CDC%252BWONDER%252BAPI%252BExample_files/CDC%252BWONDER%252BAPI%252BExample_26_0.png&quot; alt=&quot;png&quot; /&gt;&lt;/p&gt;</content><author><name>Anthony Lipphardt</name></author><summary type="html"></summary></entry></feed>